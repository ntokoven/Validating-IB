{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "13.03 toy to mnist colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "igLHoZPbUwXk",
        "outputId": "2e06ae8c-687c-436f-a263-b829c1ac76ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "from random import randint, seed\n",
        "\n",
        "# Flag to enable execution on GPU\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['randint', 'seed']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpfpprCfvc2r",
        "colab_type": "code",
        "outputId": "76ea8989-818c-428a-8edf-ddac5403cb23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emsSbOzJLwCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "train_set = MNIST('./data/MNIST', download=True, train=True, transform=ToTensor())\n",
        "test_set = MNIST('./data/MNIST', download=True, train=False, transform=ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hTHGDGBxUwXu",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "n_train_samples = len(train_set) # number of train samples\n",
        "n_test_samples = len(test_set) # number of test samples\n",
        "\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# generate samples\n",
        "seed(1234)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Initialization of the data loader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEzcIfE-UwYP",
        "colab": {}
      },
      "source": [
        "from torch.nn.functional import softplus, softmax\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden, n_classes, neg_slope=0.02):\n",
        "        super(MLP, self).__init__()\n",
        "        self.n_inputs = n_inputs,\n",
        "        self.n_hidden = n_hidden,\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        self.layers = []\n",
        "        self.num_neurons = [n_inputs] + n_hidden + [n_classes]\n",
        "        self.models = {}\n",
        "        for i in range(len(self.num_neurons) - 2):\n",
        "            self.layers.append(nn.Linear(self.num_neurons[i], self.num_neurons[i+1]))\n",
        "            self.layers.append(nn.LeakyReLU(negative_slope=neg_slope))\n",
        "            self.models['Linear{}'.format(i)] = nn.Sequential(*self.layers)\n",
        "            \n",
        "\n",
        "        self.layers.append(nn.Linear(self.num_neurons[i+1], self.num_neurons[i+2]))\n",
        "        self.models['Output'] = nn.Sequential(*self.layers)\n",
        "        self.full_model = self.models['Output']\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x, exitLayer=None): \n",
        "        if exitLayer is not None:\n",
        "            out = self.models[exitLayer](x)\n",
        "        else:\n",
        "            out = self.full_model(x)\n",
        "        return out\n",
        "'''\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.z_dim = z_dim\n",
        "        \n",
        "        # Vanilla MLP\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(10, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, z_dim*2),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1) # Flatten the input\n",
        "        params = self.net(x)\n",
        "        \n",
        "        mu, sigma = params[:,:self.z_dim], params[:,self.z_dim:]\n",
        "        sigma = softplus(sigma) + 1e-7  # Make sigma always positive\n",
        "        \n",
        "        return Independent(Normal(loc=mu, scale=sigma), 1) # Return a factorized Normal distribution\n",
        "'''\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, x_dim, y_dim, z_dim=6):\n",
        "        super(VAE, self).__init__()\n",
        "        self.K = z_dim\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Linear(x_dim, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 2*self.K))\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "                nn.Linear(self.K, y_dim))\n",
        "\n",
        "    def forward(self, x, num_sample=1):\n",
        "        if x.dim() > 2 : x = x.view(x.size(0),-1)\n",
        "\n",
        "        statistics = self.encode(x)\n",
        "        mu = statistics[:,:self.K]\n",
        "        std = softplus(statistics[:,self.K:]-5,beta=1)\n",
        "\n",
        "        encoding = self.reparametrize_n(mu, std, num_sample)\n",
        "        logit = self.decode(encoding)\n",
        "\n",
        "        if num_sample == 1 : pass\n",
        "        elif num_sample > 1 : logit = softmax(logit, dim=2).mean(0)\n",
        "\n",
        "        return (mu, std), logit, encoding\n",
        "\n",
        "    def reparametrize_n(self, mu, std, n=1):\n",
        "        # reference :\n",
        "        # http://pytorch.org/docs/0.3.1/_modules/torch/distributions.html#Distribution.sample_n\n",
        "        def expand(v):\n",
        "            if isinstance(v, Number):\n",
        "                return torch.Tensor([v]).expand(n, 1)\n",
        "            else:\n",
        "                return v.expand(n, *v.size())\n",
        "\n",
        "        if n != 1 :\n",
        "            mu = expand(mu)\n",
        "            std = expand(std)\n",
        "\n",
        "        eps = Variable(std.data.new(std.size()).normal_().to(device))\n",
        "\n",
        "        return mu + eps * std\n",
        "\n",
        "    def weight_init(self):\n",
        "        for m in self._modules:\n",
        "            xavier_init(self._modules[m])\n",
        "\n",
        "\n",
        "def xavier_init(ms):\n",
        "    for m in ms :\n",
        "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "            nn.init.xavier_uniform(m.weight,gain=nn.init.calculate_gain('relu'))\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tfwUDM2_UwYV",
        "colab": {}
      },
      "source": [
        "def get_named_layers(net):\n",
        "    conv2d_idx = 0\n",
        "    convT2d_idx = 0\n",
        "    linear_idx = 0\n",
        "    batchnorm2d_idx = 0\n",
        "    named_layers = {}\n",
        "    for mod in net.modules():\n",
        "        if isinstance(mod, torch.nn.Conv2d):\n",
        "            layer_name = 'Conv2d{}_{}-{}'.format(\n",
        "                conv2d_idx, mod.in_channels, mod.out_channels\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            conv2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.ConvTranspose2d):\n",
        "            layer_name = 'ConvT2d{}_{}-{}'.format(\n",
        "                conv2d_idx, mod.in_channels, mod.out_channels\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            convT2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.BatchNorm2d):\n",
        "            layer_name = 'BatchNorm2D{}_{}'.format(\n",
        "                batchnorm2d_idx, mod.num_features)\n",
        "            named_layers[layer_name] = mod\n",
        "            batchnorm2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.Linear):\n",
        "            layer_name = 'Linear{}_{}-{}'.format(\n",
        "                linear_idx, mod.in_features, mod.out_features\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            linear_idx += 1\n",
        "    return named_layers\n",
        "\n",
        "def accuracy(predictions, targets, mnist=True):\n",
        "    if mnist:\n",
        "        accuracy = (predictions.argmax(dim=1) == targets).type(torch.FloatTensor).mean().item()\n",
        "    else:\n",
        "        accuracy = (predictions.argmax(dim=1) == targets.argmax(dim=1)).type(torch.FloatTensor).mean().item()\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_mtR59A78PA",
        "colab_type": "code",
        "outputId": "1cf47c8a-aa1c-482a-905e-6302639a5191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Single time define the testing set. Keep it fixed until the end\n",
        "xs = []\n",
        "ys = []\n",
        "\n",
        "for x, y in test_loader:\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n",
        "\n",
        "xs = torch.cat(xs, 0)\n",
        "ys = torch.cat(ys, 0)\n",
        "\n",
        "X_test, y_test = torch.tensor(xs, requires_grad=False).flatten(start_dim=1).to(device), torch.tensor(ys, requires_grad=False).type(torch.LongTensor).to(device)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GeHMyJCMUwYZ",
        "outputId": "e11b6055-6906-420d-eab0-f03772a4430e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "dnn_hidden_units = [1024, 512, 256, 128, 64]\n",
        "dnn_input_units = X_test.shape[-1]\n",
        "dnn_output_units = 10 #y.shape[-1] if y.ndim > 1 else 1\n",
        "\n",
        "# MLP_object = VAE(dnn_input_units, dnn_output_units).to(device)\n",
        "MLP_object = MLP(dnn_input_units, dnn_hidden_units, dnn_output_units)\n",
        "get_named_layers(MLP_object)\n",
        "#MLP_object.parameters()\n",
        "#print(MLP_object.models)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Linear0_784-1024': Linear(in_features=784, out_features=1024, bias=True),\n",
              " 'Linear1_1024-512': Linear(in_features=1024, out_features=512, bias=True),\n",
              " 'Linear2_512-256': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'Linear3_256-128': Linear(in_features=256, out_features=128, bias=True),\n",
              " 'Linear4_128-64': Linear(in_features=128, out_features=64, bias=True),\n",
              " 'Linear5_64-10': Linear(in_features=64, out_features=10, bias=True)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xr1RbbhZpvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Calculate real MI throughout training\n",
        "def calc_mutual_information(hidden):\n",
        "    n_neurons = hidden.shape[-1]\n",
        "  \n",
        "    # discretization \n",
        "    n_bins = 30\n",
        "    bins = np.linspace(-1, 1, n_bins+1)\n",
        "    indices = np.digitize(hidden, bins)\n",
        "    \n",
        "    # initialize pdfs\n",
        "    pdf_x = Counter(); pdf_y = Counter(); pdf_t = Counter(); pdf_xt = Counter(); pdf_yt = Counter()\n",
        "\n",
        "    for i in range(n_train_samples):\n",
        "        pdf_x[x_train_int[i]] += 1/float(n_train_samples)\n",
        "        pdf_y[y_train[i,0]] += 1/float(n_train_samples)      \n",
        "        pdf_xt[(x_train_int[i],)+tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "        pdf_yt[(y_train[i,0],)+tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "        pdf_t[tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "    \n",
        "    # calcuate encoder mutual information I(X;T)\n",
        "    mi_xt = 0\n",
        "    for i in pdf_xt:\n",
        "        # P(x,t), P(x) and P(t)\n",
        "        p_xt = pdf_xt[i]; p_x = pdf_x[i[0]]; p_t = pdf_t[i[1:]]\n",
        "        # I(X;T)\n",
        "        mi_xt += p_xt * np.log(p_xt / p_x / p_t)\n",
        " \n",
        "    # calculate decoder mutual information I(T;Y)\n",
        "    mi_ty = 0\n",
        "    for i in pdf_yt:\n",
        "        # P(t,y), P(t) and P(y)\n",
        "        p_yt = pdf_yt[i]; p_t = pdf_t[i[1:]]; p_y = pdf_y[i[0]]\n",
        "        # I(T;Y)\n",
        "        try:\n",
        "          mi_ty += p_yt * np.log(p_yt / p_t / p_y)\n",
        "        except ZeroDivisionError:\n",
        "          mi_ty += p_yt * np.log(p_yt / (p_t + 1e-5) / (p_y + 1e-5))\n",
        "            \n",
        "    return mi_xt, mi_ty\n",
        "\n",
        "# get mutual information for all hidden layers\n",
        "def get_mutual_information(hidden):\n",
        "    mi_xt_list = []; mi_ty_list = []\n",
        "    # for hidden in hiddens:\n",
        "    if True:\n",
        "        mi_xt, mi_ty = calc_mutual_information(hidden)\n",
        "        mi_xt_list.append(mi_xt)\n",
        "        mi_ty_list.append(mi_ty)\n",
        "    return mi_xt_list, mi_ty_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_OKUCe-UwYf",
        "outputId": "7825deaa-0725-4e33-9e45-fa5e744e572e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_encoder(z_dim = 6, enc_type='MLP', layer='Linear3', weight_decay=0, num_epochs=10, eval_freq=1, beta=1):\n",
        "  \n",
        "    if enc_type == 'MLP':\n",
        "        Net = MLP(dnn_input_units, dnn_hidden_units, dnn_output_units).to(device)\n",
        "    elif enc_type =='VAE':\n",
        "        Net = VAE(dnn_input_units, dnn_output_units, z_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(Net.parameters(), lr=3e-3, weight_decay=weight_decay)#, momentum=0.2)\n",
        "\n",
        "    # X_test, Y_test = torch.tensor(x_test, requires_grad=False).float().to(device), torch.tensor(y_test, requires_grad=False).float().to(device)\n",
        "    accuracy_evaluation = {'train': [], 'test': []}\n",
        "    loss_evaluation = {'train': [], 'test': []}\n",
        "    mi_xt_all = []; mi_ty_all = []; epochs = []\n",
        "    start_time = time.time()\n",
        "    max_accuracy = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for X_train, y_train in train_loader:\n",
        "\n",
        "            X_train, y_train = X_train.flatten(start_dim=1).to(device), y_train.to(device)\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            if enc_type =='VAE':\n",
        "                (mu, std), out, z_train = Net(X_train)\n",
        "                class_loss = criterion(out, y_train).div(math.log(2))\n",
        "                info_loss = -0.5 * (1 + 2 * std.log() - mu.pow(2) - std.pow(2)).sum(1).mean().div(math.log(2))\n",
        "                loss = class_loss + beta * info_loss\n",
        "            else:\n",
        "                out = Net(X_train)\n",
        "                loss = criterion(out, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % eval_freq == 0 or epoch == num_epochs - 1:\n",
        "\n",
        "                # if enc_type =='MLP':\n",
        "                #     mi_xt, mi_ty = get_mutual_information(Net.models[layer](X_train).cpu().data.numpy())\n",
        "                # else:\n",
        "                #     mi_xt, mi_ty = get_mutual_information(z_train.cpu().data.numpy())\n",
        "                # mi_xt_all.append(mi_xt)\n",
        "                # mi_ty_all.append(mi_ty)\n",
        "\n",
        "                print('\\n'+'#'*30)\n",
        "                print('Training epoch - %d/%d' % (epoch+1, num_epochs))\n",
        "\n",
        "                if enc_type == 'VAE':\n",
        "                    (mu, std), out_test, z_test = Net(X_test)\n",
        "                    test_class_loss = criterion(out, y_train).div(math.log(2))\n",
        "                    test_info_loss = -0.5 * (1 + 2 * std.log() - mu.pow(2) - std.pow(2)).sum(1).mean().div(math.log(2))\n",
        "                    test_loss = test_class_loss + beta * test_info_loss\n",
        "                else:\n",
        "                    out_test = Net(X_test)\n",
        "                    test_loss = criterion(out_test, y_test)\n",
        "                test_accuracy = accuracy(out_test, y_test)\n",
        "                if test_accuracy > max_accuracy:\n",
        "                    max_accuracy = test_accuracy\n",
        "\n",
        "                print('Train: Accuracy - %0.3f, Loss - %0.3f' % (accuracy(out, y_train), loss))\n",
        "                print('Test: Accuracy - %0.3f, Loss - %0.3f' % (test_accuracy, test_loss))\n",
        "                # print('I(X, %s) - %s' % (layer, mi_xt))\n",
        "                # print('I(%s, Y) - %s' % (layer, mi_ty))\n",
        "                print('Elapse time: ', time.time() - start_time)\n",
        "                print('#'*30,'\\n')\n",
        "                if test_accuracy == 1 and test_loss == 0:\n",
        "                    break\n",
        "\n",
        "    return Net, max_accuracy\n",
        "\n",
        "enc_type = 'VAE'\n",
        "Encoder, performance = train_encoder(enc_type=enc_type, beta=1)#, weight_decay=0.01)\n",
        "print('Best achieved performance: ', performance)\n",
        "print(Encoder)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 57%|█████▋    | 533/938 [00:21<00:06, 63.50it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "##############################\n",
            "Training epoch - 1/10\n",
            "Train: Accuracy - 0.938, Loss - 0.430\n",
            "Test: Accuracy - 0.962, Loss - 0.438\n",
            "Elapse time:  13.203227996826172\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 2/10\n",
            "Train: Accuracy - 0.969, Loss - 0.281\n",
            "Test: Accuracy - 0.968, Loss - 0.275\n",
            "Elapse time:  26.172807931900024\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 3/10\n",
            "Train: Accuracy - 0.938, Loss - 0.455\n",
            "Test: Accuracy - 0.969, Loss - 0.458\n",
            "Elapse time:  39.14405846595764\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 4/10\n",
            "Train: Accuracy - 1.000, Loss - 0.127\n",
            "Test: Accuracy - 0.973, Loss - 0.135\n",
            "Elapse time:  52.232301235198975\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 5/10\n",
            "Train: Accuracy - 1.000, Loss - 0.162\n",
            "Test: Accuracy - 0.975, Loss - 0.163\n",
            "Elapse time:  65.23678660392761\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 6/10\n",
            "Train: Accuracy - 1.000, Loss - 0.175\n",
            "Test: Accuracy - 0.971, Loss - 0.177\n",
            "Elapse time:  78.37709927558899\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 7/10\n",
            "Train: Accuracy - 1.000, Loss - 0.123\n",
            "Test: Accuracy - 0.972, Loss - 0.119\n",
            "Elapse time:  91.25242805480957\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 8/10\n",
            "Train: Accuracy - 1.000, Loss - 0.132\n",
            "Test: Accuracy - 0.971, Loss - 0.131\n",
            "Elapse time:  104.2206802368164\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 9/10\n",
            "Train: Accuracy - 1.000, Loss - 0.114\n",
            "Test: Accuracy - 0.970, Loss - 0.121\n",
            "Elapse time:  117.19670128822327\n",
            "############################## \n",
            "\n",
            "\n",
            "##############################\n",
            "Training epoch - 10/10\n",
            "Train: Accuracy - 0.969, Loss - 0.248\n",
            "Test: Accuracy - 0.973, Loss - 0.250\n",
            "Elapse time:  130.19382214546204\n",
            "############################## \n",
            "\n",
            "Best achieved performance:  0.9746999740600586\n",
            "VAE(\n",
            "  (encode): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=1024, out_features=12, bias=True)\n",
            "  )\n",
            "  (decode): Sequential(\n",
            "    (0): Linear(in_features=6, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z3qZ1W8XUwYr",
        "colab": {}
      },
      "source": [
        "# Neural approximation of MI starts here\n",
        "# Auxiliary network for mutual information estimation\n",
        "class MIEstimator(nn.Module):\n",
        "    def __init__(self, size1, size2):\n",
        "        super(MIEstimator, self).__init__()\n",
        "        \n",
        "        # Vanilla MLP\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(size1 + size2, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "    \n",
        "    # Gradient for JSD mutual information estimation and EB-based estimation\n",
        "    def forward(self, x1, x2):\n",
        "        # breakpoint()\n",
        "        pos = self.net(torch.cat([x1, x2], 1)) #Positive Samples \n",
        "        neg = self.net(torch.cat([torch.roll(x1, 1, 0), x2], 1)) #Predictions for shuffled (negative) samples from p(z1)p(z2)\n",
        "        #breakpoint()\n",
        "        return -softplus(-pos).mean() - softplus(neg).mean(), pos.mean() - neg.exp().mean() + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rHCT2r8YuyO",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Subset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class Scheduler:\n",
        "    def __call__(self, **kwargs):\n",
        "        raise NotImplemented()\n",
        "\n",
        "class LinearScheduler(Scheduler):\n",
        "    def __init__(self, start_value, end_value, n_iterations, start_iteration=0):\n",
        "        self.start_value = start_value\n",
        "        self.end_value = end_value\n",
        "        self.n_iterations = n_iterations\n",
        "        self.start_iteration = start_iteration\n",
        "        self.m = (end_value - start_value) / n_iterations\n",
        "\n",
        "    def __call__(self, iteration):\n",
        "        if iteration > self.start_iteration + self.n_iterations:\n",
        "            return self.end_value\n",
        "        elif iteration <= self.start_iteration:\n",
        "            return self.start_value\n",
        "        else:\n",
        "            return (iteration - self.start_iteration) * self.m + self.start_value\n",
        "\n",
        "class ExponentialScheduler(LinearScheduler):\n",
        "    def __init__(self, start_value, end_value, n_iterations, start_iteration=0, base=10):\n",
        "        self.base = base\n",
        "\n",
        "        super(ExponentialScheduler, self).__init__(start_value=math.log(start_value, base),\n",
        "                                                   end_value=math.log(end_value, base),\n",
        "                                                   n_iterations=n_iterations,\n",
        "                                                   start_iteration=start_iteration)\n",
        "\n",
        "    def __call__(self, iteration):\n",
        "        linear_value = super(ExponentialScheduler, self).__call__(iteration)\n",
        "        return self.base ** linear_value\n",
        "\n",
        "def split(dataset, size, split_type):\n",
        "    if split_type == 'Random':\n",
        "        data_split, _ = torch.utils.data.random_split(dataset, [size, len(dataset) - size])\n",
        "    elif split_type == 'Balanced':\n",
        "        class_ids = {}\n",
        "        for idx, (_, y) in enumerate(dataset):\n",
        "            if y not in class_ids:\n",
        "                class_ids[y] = []\n",
        "            class_ids[y].append(idx)\n",
        "\n",
        "        ids_per_class = size // len(class_ids)\n",
        "\n",
        "        selected_ids = []\n",
        "\n",
        "        for ids in class_ids.values():\n",
        "            selected_ids += list(np.random.choice(ids, min(ids_per_class, len(ids)), replace=False))\n",
        "        data_split = torch.utils.data.Subset(dataset, selected_ids)\n",
        "\n",
        "    return data_split\n",
        "\n",
        "\n",
        "class EmbeddedDataset:\n",
        "    BLOCK_SIZE = 256\n",
        "\n",
        "    def __init__(self, base_dataset, encoder, enc_type, cuda=True):\n",
        "        if cuda:\n",
        "            encoder = encoder.cuda()\n",
        "        self.means, self.target = self._embed(encoder, base_dataset, cuda)\n",
        "\n",
        "    def _embed(self, encoder, dataset, cuda):\n",
        "        encoder.eval()\n",
        "\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.BLOCK_SIZE,\n",
        "            shuffle=False)\n",
        "\n",
        "        ys = []\n",
        "        reps = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in data_loader:\n",
        "                x = x.flatten(start_dim=1)\n",
        "                if cuda:\n",
        "                    x = x.cuda()\n",
        "                    y = y.cuda()\n",
        "                if enc_type == 'VAE':\n",
        "                    (_, _), _, p_z_given_x = encoder(x)\n",
        "                else: \n",
        "                    p_z_given_x = encoder(x)\n",
        "                reps.append(p_z_given_x.detach())\n",
        "                ys.append(y)\n",
        "            ys = torch.cat(ys, 0)\n",
        "\n",
        "        encoder.train()\n",
        "        return reps, ys\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        y = self.target[index]\n",
        "        # x = self.means[index][0]\n",
        "        x = self.means[index // self.BLOCK_SIZE][index % self.BLOCK_SIZE]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target.size(0)\n",
        "\n",
        "def train_and_evaluate_linear_model(train_set, test_set, solver='saga', multi_class='multinomial', tol=.1, C=2):\n",
        "    model = LogisticRegression(solver=solver, multi_class=multi_class, tol=tol, C=C)\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    x_train, y_train = build_matrix(train_set)\n",
        "    x_test, y_test = build_matrix(test_set)\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    test_accuracy = model.score(x_test, y_test)\n",
        "    train_accuracy = model.score(x_train, y_train)\n",
        "\n",
        "    return train_accuracy, test_accuracy\n",
        "\n",
        "\n",
        "def build_matrix(dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    xs = torch.cat(xs, 0)\n",
        "    ys = torch.cat(ys, 0)\n",
        "\n",
        "    if xs.is_cuda:\n",
        "        xs = xs.cpu()\n",
        "    if ys.is_cuda:\n",
        "        ys = ys.cpu()\n",
        "\n",
        "    return xs.data.numpy(), ys.data.numpy()\n",
        "\n",
        "def evaluate(encoder, enc_type, train_on, test_on, cuda):\n",
        "    embedded_train = EmbeddedDataset(train_on, encoder, enc_type, cuda=cuda)\n",
        "    embedded_test = EmbeddedDataset(test_on, encoder, enc_type, cuda=cuda)\n",
        "    return train_and_evaluate_linear_model(embedded_train, embedded_test)\n",
        "\n",
        "def onehot_encoding(x, num_classes=10):\n",
        "    x_onehot = torch.FloatTensor(x.shape[0], num_classes)\n",
        "    x_onehot.zero_()\n",
        "    x_onehot.scatter_(1, x.view(x.shape[0], 1), 1)\n",
        "    return x_onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91duTE0CUrud",
        "colab_type": "code",
        "outputId": "40e76540-44aa-45c8-9086-393bcaf971ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def train_MI(encoder, beta=None, num_epochs=2000, estimate_mi_on_train=True):\n",
        "    \n",
        "    if estimate_mi_on_train:\n",
        "        loader = train_loader\n",
        "    else:\n",
        "        loader = test_loader\n",
        "\n",
        "    if enc_type == 'VAE':\n",
        "       (_, _), _, z_test = encoder(X_test)\n",
        "    else:\n",
        "       z_test = encoder(X_test)\n",
        "    z_test = torch.tensor(z_test, requires_grad=False).to(device)\n",
        "\n",
        "    x_dim, y_dim, z_dim = X_test.shape[-1], dnn_output_units, z_test.shape[-1]\n",
        "    print(x_dim, y_dim, z_dim)\n",
        "    mi_estimator_X = MIEstimator(x_dim, z_dim).to(device)\n",
        "    mi_estimator_Y = MIEstimator(z_dim, y_dim).to(device)\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "    {'params': mi_estimator_X.parameters(), 'lr':1e-4},\n",
        "    {'params': mi_estimator_Y.parameters(), 'lr':1e-4},\n",
        "    ])\n",
        "    if beta is None:\n",
        "        use_scheduler = True\n",
        "        beta_scheduler = ExponentialScheduler(start_value=1e-9, end_value=1e-1, n_iterations=500, start_iteration=1)\n",
        "    else:\n",
        "        use_scheduler = False\n",
        "\n",
        "    start_time = time.time()\n",
        "    max_MI_x, max_MI_y = 0, 0\n",
        "    mi_mean_est_all = {'X': [], 'Y': []}\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        if use_scheduler:\n",
        "            beta = beta_scheduler(epoch)\n",
        "        mi_over_epoch = {'X': [], 'Y': []}\n",
        "        for X, y in loader:\n",
        "            y = onehot_encoding(y)\n",
        "            X, y = X.flatten(start_dim=1).to(device), y.float().to(device)\n",
        "\n",
        "            if enc_type == 'VAE':\n",
        "                (_, _), _, z = encoder(X)\n",
        "            else:\n",
        "                z = encoder(X)\n",
        "            optimizer.zero_grad()\n",
        "            mi_gradient_X, mi_estimation_X = mi_estimator_X(X, z)\n",
        "            mi_gradient_X = mi_gradient_X.mean()\n",
        "            mi_estimation_X = mi_estimation_X.mean()\n",
        "\n",
        "            mi_gradient_Y, mi_estimation_Y = mi_estimator_Y(z, y)\n",
        "            mi_gradient_Y = mi_gradient_Y.mean()\n",
        "            mi_estimation_Y = mi_estimation_Y.mean()\n",
        "                    \n",
        "            loss_mi = - mi_gradient_Y - beta * mi_gradient_X\n",
        "            loss_mi.backward()\n",
        "            optimizer.step()\n",
        "            mi_over_epoch['X'].append(mi_estimation_X.item())\n",
        "            mi_over_epoch['Y'].append(mi_estimation_Y.item())\n",
        "            if mi_estimation_X.item() > max_MI_x and epoch > 5:#10:\n",
        "                max_MI_x = mi_estimation_X.item()\n",
        "                max_MI_y = mi_estimation_Y.item()\n",
        "        mi_mean_est_all['X'].append(np.mean(mi_over_epoch['X']))\n",
        "        mi_mean_est_all['Y'].append(np.mean(mi_over_epoch['Y']))\n",
        "        eval_freq = 1\n",
        "            \n",
        "        # if epoch >= 50 and np.mean(mi_est_all['X'][-50]) > max_MI_x - 1e-1:\n",
        "        #     break\n",
        "        if epoch % eval_freq == 0 or epoch == num_epochs - 1:\n",
        "            #if mi_estimation_X.item() > max_MI_x and epoch > 5:\n",
        "            #    max_MI_x = mi_estimation_X.item()\n",
        "            #    max_MI_y = mi_estimation_Y.item()\n",
        "            \n",
        "            # if epoch >= 15 and np.mean(mi_est_all['X'][-10]) > max_MI_x - 1e-1:\n",
        "            #     break\n",
        "\n",
        "            print('#'*30)\n",
        "            print('Step - ', epoch)\n",
        "            print('Beta - ', beta)\n",
        "                \n",
        "            # print('Mean MI X', np.mean(mi_mean_est_all['X']))\n",
        "            # print('Mean MI Y', np.mean(mi_mean_est_all['Y']))\n",
        "            if epoch >= 1:\n",
        "\n",
        "              delta_x = np.abs(mi_mean_est_all['X'][-2] - mi_mean_est_all['X'][-1])\n",
        "              print('Delta X: ', delta_x)\n",
        "              \n",
        "              delta_y = np.abs(mi_mean_est_all['Y'][-2] - mi_mean_est_all['Y'][-1])\n",
        "              print('Delta Y: ', delta_y)\n",
        "              print('\\nMean MI X for last 10', mi_mean_est_all['X'][-10:])\n",
        "              print('Mean MI Y for last 10', mi_mean_est_all['Y'][-10:])\n",
        "              mi_df = pd.DataFrame.from_dict(mi_mean_est_all)\n",
        "              mi_df.to_csv('mi_mlp.csv', sep=' ')\n",
        "            # print('I_est(X, %s) - %s' % (layer, mi_estimation_X.item()))\n",
        "            #print('Grad I_est(X, T)', mi_gradient_X)\n",
        "            # print('I_est(%s, Y) - %s' % (layer, mi_estimation_Y.item()))\n",
        "            # print('Max I_est(X, %s) - %s' % (layer, max_MI_x))\n",
        "            #print('Grad I_est(X, T)', mi_gradient_X)\n",
        "            # print('Max I_est(%s, Y) - %s' % (layer, max_MI_y))\n",
        "            #print('Grad I_est(T, Y)', mi_gradient_Y)\n",
        "            print('Elapsed time training MI for %s: %s' % (layer, time.time() - start_time))\n",
        "            print('#'*30,'\\n')\n",
        "\n",
        "            \n",
        "    return max_MI_x, max_MI_y\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Initialization of the data loader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "num_classes = 10\n",
        "labels_per_class = [2**i for i in range(0, int(np.log2(n_train_samples/num_classes)))]\n",
        "num_labels_range = num_classes * np.array(labels_per_class)\n",
        "# layers = ['Linear2', 'Linear3', 'Linear4']\n",
        "layers = ['Linear4']\n",
        "if enc_type == 'MLP':   \n",
        "    acc = {layer:{i:[] for i in num_labels_range} for layer in layers}\n",
        "else:\n",
        "    acc = {i:[] for i in num_labels_range}\n",
        "print(num_labels_range)\n",
        "seeds = [9]#, 42, 103, 48, 79]\n",
        "mi_layers = {} #probably make for different seeds as well\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "\n",
        "    \n",
        "    print('\\nRunning with seed %d out of %d' % (i+1, len(seeds)))\n",
        "    torch.manual_seed(seeds[i])\n",
        "    np.random.seed(seeds[i])\n",
        "    seed(seeds[i])\n",
        "    \n",
        "    \n",
        "    for layer in layers:\n",
        "        '''\n",
        "        if seeds[i] == 9:\n",
        "            if enc_type == 'MLP':\n",
        "                MI_X, MI_Y = train_MI(Encoder.models[layer], beta=1)\n",
        "            else:\n",
        "                MI_X, MI_Y = train_MI(Encoder, num_epochs=2000)\n",
        "            mi_layers[layer] = (MI_X, MI_Y)\n",
        "            print('MI values for %s - %s, %s' % (layer, MI_X, MI_Y))\n",
        "        '''\n",
        "        # train_set = torch.utils.data.TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device)) \n",
        "        # test_set = torch.utils.data.TensorDataset(torch.Tensor(x_test).to(device), torch.Tensor(y_test).to(device))\n",
        "    \n",
        "        for num_labels in num_labels_range:\n",
        "            print('\\n\\nEvaluating for %s (labels per class - %s)' % (layer, num_labels))\n",
        "            #acc[layer].append(train_MI(num_labels, Encoder, num_epochs=2000))\n",
        "            print('Num labels:', num_labels)\n",
        "            train_subset = split(train_set, num_labels, 'Balanced') \n",
        "            print(len(train_subset))\n",
        "            # test_subset = split(train_set, n_test_samples, 'Random') \n",
        "            if enc_type == 'MLP':\n",
        "                train_accuracy, test_accuracy = evaluate(encoder=Encoder.models[layer], enc_type=enc_type, train_on=train_subset, test_on=test_set, cuda=torch.cuda.is_available())\n",
        "            else:\n",
        "                train_accuracy, test_accuracy = evaluate(encoder=Encoder, enc_type=enc_type, train_on=train_subset, test_on=test_set, cuda=torch.cuda.is_available())\n",
        "            #train_accuracy, test_accuracy = train_and_evaluate_linear_model(train_subset, test_set, C=2)\n",
        "            print('Train Accuracy: %f'% train_accuracy)\n",
        "            print('Test Accuracy: %f'% test_accuracy)\n",
        "            if enc_type == 'MLP':\n",
        "                acc[layer][num_labels].append(test_accuracy)\n",
        "            else:\n",
        "                acc[num_labels].append(test_accuracy)\n",
        "\n",
        "    print('Elapsed time - ', time.time() - start_time)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   10    20    40    80   160   320   640  1280  2560  5120 10240 20480]\n",
            "\n",
            "Running with seed 1 out of 1\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 10)\n",
            "Num labels: 10\n",
            "10\n",
            "Train Accuracy: 1.000000\n",
            "Test Accuracy: 0.929800\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 20)\n",
            "Num labels: 20\n",
            "20\n",
            "Train Accuracy: 1.000000\n",
            "Test Accuracy: 0.936100\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 40)\n",
            "Num labels: 40\n",
            "40\n",
            "Train Accuracy: 1.000000\n",
            "Test Accuracy: 0.967800\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 80)\n",
            "Num labels: 80\n",
            "80\n",
            "Train Accuracy: 0.987500\n",
            "Test Accuracy: 0.970600\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 160)\n",
            "Num labels: 160\n",
            "160\n",
            "Train Accuracy: 0.993750\n",
            "Test Accuracy: 0.973700\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 320)\n",
            "Num labels: 320\n",
            "320\n",
            "Train Accuracy: 0.993750\n",
            "Test Accuracy: 0.971000\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 640)\n",
            "Num labels: 640\n",
            "640\n",
            "Train Accuracy: 0.992188\n",
            "Test Accuracy: 0.973700\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 1280)\n",
            "Num labels: 1280\n",
            "1280\n",
            "Train Accuracy: 0.993750\n",
            "Test Accuracy: 0.973300\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 2560)\n",
            "Num labels: 2560\n",
            "2560\n",
            "Train Accuracy: 0.992188\n",
            "Test Accuracy: 0.972100\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 5120)\n",
            "Num labels: 5120\n",
            "5120\n",
            "Train Accuracy: 0.990625\n",
            "Test Accuracy: 0.975500\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 10240)\n",
            "Num labels: 10240\n",
            "10240\n",
            "Train Accuracy: 0.993262\n",
            "Test Accuracy: 0.974100\n",
            "\n",
            "\n",
            "Evaluating for Linear4 (labels per class - 20480)\n",
            "Num labels: 20480\n",
            "20480\n",
            "Train Accuracy: 0.993164\n",
            "Test Accuracy: 0.974200\n",
            "Elapsed time -  108.7633318901062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-P5T0Jg_yEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2056727a-0f54-4790-b5f7-334a1b4eedbe"
      },
      "source": [
        "performance"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9746999740600586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YG63TT4AHFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_metric(best_performance, accuracy):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOs0siAkTKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mi_df_plot = pd.read_csv('mi_mlp.csv', sep=' ', index_col=0)\n",
        "# mi_df_plot[mi_df_plot < 0] = np.nan\n",
        "# plt.plot(np.arange(len(mi_df_plot)), mi_df_plot['X'], label='I(X,Z)')\n",
        "# plt.plot(np.arange(len(mi_df_plot)), mi_df_plot['Y'], label='I(Z,Y)')\n",
        "# plt.legend()\n",
        "# plt.savefig('mi.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJDUb6tisON",
        "colab_type": "code",
        "outputId": "90b35165-070c-440e-cc00-8fd6f9189f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDXAwdvsKrz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "acc_df = pd.DataFrame.from_dict(acc)\n",
        "plot_data = pd.DataFrame()\n",
        "# acc_df.to_csv('acc_mlp_l2.csv', sep=' ')\n",
        "acc_df = acc_df.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xgel0Y-Cl17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e75486a5-945b-4afb-a4e8-b05a7243ec71"
      },
      "source": [
        "acc_df[0]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10       0.9298\n",
              "20       0.9361\n",
              "40       0.9678\n",
              "80       0.9706\n",
              "160      0.9737\n",
              "320      0.9710\n",
              "640      0.9737\n",
              "1280     0.9733\n",
              "2560     0.9721\n",
              "5120     0.9755\n",
              "10240    0.9741\n",
              "20480    0.9742\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L2TlkyAgJis",
        "colab_type": "code",
        "outputId": "3bf42969-2754-4d45-e87c-8f412b316dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "source": [
        "mean = lambda x: np.mean(x)\n",
        "std = lambda x: np.std(x)\n",
        "acc_df.apply(mean)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Linear4'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-c642f6d484ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Linear4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Linear4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMtPl7e7e1v4",
        "colab_type": "code",
        "outputId": "816c1278-0556-4cbb-9ce9-dcabe99a919e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "pd.DataFrame.from_dict(mi_layers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Linear2</th>\n",
              "      <th>Linear3</th>\n",
              "      <th>Linear4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.838330</td>\n",
              "      <td>2.027251</td>\n",
              "      <td>1.946483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.697114</td>\n",
              "      <td>0.691475</td>\n",
              "      <td>0.689363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Linear2   Linear3   Linear4\n",
              "0  2.838330  2.027251  1.946483\n",
              "1  0.697114  0.691475  0.689363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUbkkzYOmqI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "6f652791-b3ce-498d-bfbf-3809c5044945"
      },
      "source": [
        "np.log10(nums)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-3482eb4cc5cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nums' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqS3_Be1nDrG",
        "colab_type": "code",
        "outputId": "e8c538fa-aa3c-43b7-8c1e-a5acac463ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "from shapely.geometry import Polygon\n",
        "\n",
        "fig1, ax1 = plt.subplots(1, 1, sharex=True)\n",
        "nums = np.array(num_labels_range)\n",
        "colors = ['black', 'blue', 'red']\n",
        "# for i in range(len(layers)):\n",
        "#     means = acc_df[layers[i]].apply(mean)\n",
        "#     stds = acc_df[layers[i]].apply(std)\n",
        "#     ax1[i].plot(np.log10(nums), means, color=colors[i], label=layers[i]+' + L2 (lambda = 0.01)')\n",
        "#     ax1[i].fill_between(np.log10(nums), means - stds, means + stds, facecolor=colors[i], interpolate=True, alpha=0.25)\n",
        "#     ax1[i].grid()\n",
        "\n",
        "for i in range(1):#len(layers)):\n",
        "    means = acc_df[0].apply(mean)\n",
        "    stds = acc_df[0].apply(std)\n",
        "    ax1.plot(np.log10(nums), means, color=colors[i])\n",
        "    ax1.plot(np.log10(nums), performance * np.ones(len(nums)))\n",
        "    #ax1[i].fill_between(np.log10(nums), means - stds, means + stds, facecolor=colors[i], interpolate=True, alpha=0.25)\n",
        "    ax1.fill_between(np.log10(nums), means, performance * np.ones(len(nums)), facecolor=colors[i], interpolate=True, alpha=0.25)\n",
        "    ax1.grid()\n",
        "# ax1[-1].set_xlabel('# Labels')\n",
        "# ax1[1].set_ylabel('Accuracy')\n",
        "\n",
        "fig1.legend()\n",
        "fig1.set_size_inches(10, 7, forward=True)\n",
        "fig1.savefig('acc_beta1.png')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApwAAAHTCAYAAACKrciBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhRMAAACu+v9B+wZYMSYP9gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLng22crFsJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22b62604-a665-4a23-82a8-9f9d0fbdfe9a"
      },
      "source": [
        "acc_df[0].to_numpy()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9298, 0.9361, 0.9678, 0.9706, 0.9737, 0.971 , 0.9737, 0.9733,\n",
              "       0.9721, 0.9755, 0.9741, 0.9742])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmyiiKIoE4z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ce6cd38-8bd2-4f95-bf52-1819fa08a94d"
      },
      "source": [
        "perf = performance * np.ones(len(nums))\n",
        "def target_metric(best_performance, accuracy_over_labels, nums):\n",
        "    perf = best_performance * np.ones(len(nums))\n",
        "    x_y_curve1 = [(np.log10(nums)[i], accuracy_over_labels[i]) for i in range(len(nums))]#[(1,1),(2,1),(3,3),(4,3)] #these are your points for curve 1 \n",
        "    x_y_curve2 = [(np.log10(nums)[i], perf[i]) for i in range(len(nums))] #these are your points for curve 2 \n",
        "\n",
        "    polygon_points = [] #creates a empty list where we will append the points to create the polygon\n",
        "\n",
        "    for xyvalue in x_y_curve1:\n",
        "        polygon_points.append([xyvalue[0],xyvalue[1]]) #append all xy points for curve 1\n",
        "\n",
        "    for xyvalue in x_y_curve2[::-1]:\n",
        "        polygon_points.append([xyvalue[0],xyvalue[1]]) #append all xy points for curve 2 in the reverse order (from last point to first point)\n",
        "\n",
        "    for xyvalue in x_y_curve1[0:1]:\n",
        "        polygon_points.append([xyvalue[0],xyvalue[1]]) #append the first point in curve 1 again, to it \"closes\" the polygon\n",
        "\n",
        "    polygon = Polygon(polygon_points)\n",
        "    area = polygon.area\n",
        "    return area\n",
        "\n",
        "def accuracy_at_percent(percentage):\n",
        "    num_labels = percentage * len(train_set)\n",
        "    print(num_labels)\n",
        "\n",
        "metric_value = target_metric(performance, acc_df[0].to_numpy(), nums)\n",
        "\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.024624167749608718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-JjG7v4EVKT",
        "colab_type": "code",
        "outputId": "3c4a9290-134e-4169-9c3e-c66fb710134a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "mi_df = pd.DataFrame.from_dict(mi_layers)\n",
        "fig2, ax2 = plt.subplots(1, 1, sharex=True)\n",
        "colors = ['black', 'blue', 'red']\n",
        "for i in range(len(layers)):\n",
        "    ax2.scatter(mi_df.loc[0, layers[i]], mi_df.loc[1, layers[i]], color=colors[i], label=layers[i])\n",
        "    \n",
        "    ax2.annotate(layers[i], (mi_df.loc[0, layers[i]], mi_df.loc[1, layers[i]]))\n",
        "    ax2.grid()\n",
        "ax2.set_xlabel('I(X, Z)')\n",
        "ax2.set_ylabel('I(Z, Y)')\n",
        "\n",
        "fig2.legend()\n",
        "fig2.set_size_inches(10, 7, forward=True)\n",
        "fig2.savefig('mi_l2.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArEAAAHhCAYAAAB5vOZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+X9bmSbhdqcoIQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoFtMDUkoLi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}