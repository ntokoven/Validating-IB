{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "03.03 Copy of information_bottleneck.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b2MZ3QFDUwXT"
      },
      "source": [
        "# Information Bottleneck theory for Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xe77ybVGUwXZ"
      },
      "source": [
        "This is a demonstration of the information bottleneck theory for deep learning, introduced by Naftali Tishby. Here I tried to reproduce the main results in their recent paper [Opening the black box of Deep Neural Networks via Information](https://arxiv.org/pdf/1703.00810.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PG-Wfp0DUwXc"
      },
      "source": [
        "## Data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KNmtaAb7UwXf"
      },
      "source": [
        "First, we will generate a very simple dataset for the demonstration. The inputs are vectors of 10 binaries, and the outputs are just single binaries. The inputs could be represented by integers from 0 to 1023 ($=2^{10}-1$). The 1024 possible inputs are divided into 16 groups (each group has 64 numbers), and each integer input $n\\in[0,1023]$ belongs to group $i$ if $x\\equiv i \\pmod{16}$, where $i \\in [0,15]$. Each group $i$ is then associated with a random binary number - we build kinda of a distribution over space of possible discrete states (output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "igLHoZPbUwXk",
        "outputId": "dea91f8b-ecd3-490d-df7d-d9761e4f686b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "from random import randint, seed\n",
        "\n",
        "# Flag to enable execution on GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hTHGDGBxUwXu",
        "outputId": "12f16a84-4e5e-42c1-9645-29d07f156874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_train_samples = 50000 # number of train samples\n",
        "n_test_samples = 10000 # number of test samples\n",
        "\n",
        "groups = np.append(np.zeros(8),np.ones(8)) # 16 groups\n",
        "print(groups)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "np.random.shuffle(groups)\n",
        "\n",
        "# generate samples\n",
        "seed(1234)\n",
        "def generate_samples(n_samples):\n",
        "    x_data = np.zeros((n_samples, 10)) # inputs\n",
        "    x_int = np.zeros(n_samples) # integers representing the inputs\n",
        "    y_data = np.zeros((n_samples, 2)) # outputs\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        random_int = randint(0, 1023)\n",
        "        x_data[i,:] = [int(b) for b in list(\"{0:b}\".format(random_int).zfill(10))]\n",
        "        x_int[i] = random_int\n",
        "        y_data[i,0] = groups[random_int % 16]\n",
        "        y_data[i,1] = 1 - y_data[i,0]\n",
        "        \n",
        "    return x_data, y_data, x_int\n",
        "\n",
        "x_train, y_train, x_train_int = generate_samples(n_train_samples) # training dataset\n",
        "x_test, y_test, _ = generate_samples(n_test_samples) # testing dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ai6D9irwUwX_",
        "outputId": "193e472f-c0df-4362-b2e0-efc09b13976f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "d={}\n",
        "for i in y_train:\n",
        "    d[str(i[0])+', '+str(i[1])] = 0\n",
        "for i in y_train:\n",
        "    d[str(i[0])+', '+str(i[1])] += 1\n",
        "print('Distribution of classes: ', d)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution of classes:  {'1.0, 0.0': 24820, '0.0, 1.0': 25180}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-wDq6HyUwYI"
      },
      "source": [
        "For our dataset, the theoritical mutual information between $X$ and $Y$ would be\n",
        "\\begin{align}\n",
        "I(X;Y) & = \\sum_{x\\in X, y\\in Y}P(x,y)\\log\\Big(\\frac{P(x,y)}{P(x)P(y)}\\Big) \\\\\n",
        "& = \\sum_{x\\in X}\\Big[P(x,y=0)\\log\\Big(\\frac{P(x,y=0)}{P(x)P(y=0)}\\Big) + P(x,y=1)\\log\\Big(\\frac{P(x,y=1)}{P(x)P(y=1)}\\Big)\\Big] \\\\\n",
        "& = 1024 \\Big[ \\frac{1}{1024}\\log\\Big(\\frac{1/1024}{0.5/1024}\\Big) + 0\\Big] \\\\\n",
        "& = 0.693.\n",
        "\\end{align}\n",
        "Note that terms with $P(x,y)=0$ are set to $0$ for entropy calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZEzcIfE-UwYP",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden, n_classes, neg_slope=0.02):\n",
        "        super(MLP, self).__init__()\n",
        "        self.n_inputs = n_inputs,\n",
        "        self.n_hidden = n_hidden,\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        self.layers = []\n",
        "        self.num_neurons = [n_inputs] + n_hidden + [n_classes]\n",
        "        self.models = {}\n",
        "        for i in range(len(self.num_neurons) - 2):\n",
        "            self.layers.append(nn.Linear(self.num_neurons[i], self.num_neurons[i+1]))\n",
        "            self.layers.append(nn.Tanh())\n",
        "            self.models['Linear{}'.format(i)] = nn.Sequential(*self.layers)\n",
        "            \n",
        "\n",
        "        self.layers.append(nn.Linear(self.num_neurons[i+1], self.num_neurons[i+2]))\n",
        "        self.models['Output'] = nn.Sequential(*self.layers)\n",
        "        self.full_model = self.models['Output']\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x, exitLayer=None): \n",
        "        if exitLayer is not None:\n",
        "            out = self.models[exitLayer](x)\n",
        "        else:\n",
        "            out = self.full_model(x)\n",
        "        return out\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.z_dim = z_dim\n",
        "        \n",
        "        # Vanilla MLP\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(10, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, z_dim*2),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1) # Flatten the input\n",
        "        params = self.net(x)\n",
        "        \n",
        "        mu, sigma = params[:,:self.z_dim], params[:,self.z_dim:]\n",
        "        sigma = softplus(sigma) + 1e-7  # Make sigma always positive\n",
        "        \n",
        "        return Independent(Normal(loc=mu, scale=sigma), 1) # Return a factorized Normal distribution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tfwUDM2_UwYV",
        "colab": {}
      },
      "source": [
        "def get_named_layers(net):\n",
        "    conv2d_idx = 0\n",
        "    convT2d_idx = 0\n",
        "    linear_idx = 0\n",
        "    batchnorm2d_idx = 0\n",
        "    named_layers = {}\n",
        "    for mod in net.modules():\n",
        "        if isinstance(mod, torch.nn.Conv2d):\n",
        "            layer_name = 'Conv2d{}_{}-{}'.format(\n",
        "                conv2d_idx, mod.in_channels, mod.out_channels\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            conv2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.ConvTranspose2d):\n",
        "            layer_name = 'ConvT2d{}_{}-{}'.format(\n",
        "                conv2d_idx, mod.in_channels, mod.out_channels\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            convT2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.BatchNorm2d):\n",
        "            layer_name = 'BatchNorm2D{}_{}'.format(\n",
        "                batchnorm2d_idx, mod.num_features)\n",
        "            named_layers[layer_name] = mod\n",
        "            batchnorm2d_idx += 1\n",
        "        elif isinstance(mod, torch.nn.Linear):\n",
        "            layer_name = 'Linear{}_{}-{}'.format(\n",
        "                linear_idx, mod.in_features, mod.out_features\n",
        "            )\n",
        "            named_layers[layer_name] = mod\n",
        "            linear_idx += 1\n",
        "    return named_layers\n",
        "\n",
        "def accuracy(predictions, targets):\n",
        "    accuracy = (predictions.argmax(dim=1) == targets.argmax(dim=1)).type(torch.FloatTensor).mean().item()\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GeHMyJCMUwYZ",
        "outputId": "d5471fca-ca52-44b8-bafc-277435638484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "num_epochs = 2000\n",
        "dnn_hidden_units = [16, 12, 8, 6, 4]\n",
        "dnn_input_units = x_train.shape[1]\n",
        "dnn_output_units = y_train.shape[1]\n",
        "eval_freq = 100\n",
        "\n",
        "MLP_object = MLP(dnn_input_units, dnn_hidden_units, dnn_output_units).to(device)\n",
        "get_named_layers(MLP_object)\n",
        "#MLP_object.parameters()\n",
        "print(MLP_object.models)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Linear0': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "), 'Linear1': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "  (3): Tanh()\n",
            "), 'Linear2': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (5): Tanh()\n",
            "), 'Linear3': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (5): Tanh()\n",
            "  (6): Linear(in_features=8, out_features=6, bias=True)\n",
            "  (7): Tanh()\n",
            "), 'Linear4': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (5): Tanh()\n",
            "  (6): Linear(in_features=8, out_features=6, bias=True)\n",
            "  (7): Tanh()\n",
            "  (8): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (9): Tanh()\n",
            "), 'Output': Sequential(\n",
            "  (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=12, out_features=8, bias=True)\n",
            "  (5): Tanh()\n",
            "  (6): Linear(in_features=8, out_features=6, bias=True)\n",
            "  (7): Tanh()\n",
            "  (8): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (9): Tanh()\n",
            "  (10): Linear(in_features=4, out_features=2, bias=True)\n",
            ")}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xr1RbbhZpvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Calculate real MI throughout training\n",
        "def calc_mutual_information(hidden):\n",
        "    n_neurons = hidden.shape[-1]\n",
        "  \n",
        "    # discretization \n",
        "    n_bins = 30\n",
        "    bins = np.linspace(-1, 1, n_bins+1)\n",
        "    indices = np.digitize(hidden, bins)\n",
        "    \n",
        "    # initialize pdfs\n",
        "    pdf_x = Counter(); pdf_y = Counter(); pdf_t = Counter(); pdf_xt = Counter(); pdf_yt = Counter()\n",
        "\n",
        "    for i in range(n_train_samples):\n",
        "        pdf_x[x_train_int[i]] += 1/float(n_train_samples)\n",
        "        pdf_y[y_train[i,0]] += 1/float(n_train_samples)      \n",
        "        pdf_xt[(x_train_int[i],)+tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "        pdf_yt[(y_train[i,0],)+tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "        pdf_t[tuple(indices[i,:])] += 1/float(n_train_samples)\n",
        "    \n",
        "    # calcuate encoder mutual information I(X;T)\n",
        "    mi_xt = 0\n",
        "    for i in pdf_xt:\n",
        "        # P(x,t), P(x) and P(t)\n",
        "        p_xt = pdf_xt[i]; p_x = pdf_x[i[0]]; p_t = pdf_t[i[1:]]\n",
        "        # I(X;T)\n",
        "        mi_xt += p_xt * np.log(p_xt / p_x / p_t)\n",
        " \n",
        "    # calculate decoder mutual information I(T;Y)\n",
        "    mi_ty = 0\n",
        "    for i in pdf_yt:\n",
        "        # P(t,y), P(t) and P(y)\n",
        "        p_yt = pdf_yt[i]; p_t = pdf_t[i[1:]]; p_y = pdf_y[i[0]]\n",
        "        # I(T;Y)\n",
        "        try:\n",
        "          mi_ty += p_yt * np.log(p_yt / p_t / p_y)\n",
        "        except ZeroDivisionError:\n",
        "          mi_ty += p_yt * np.log(p_yt / (p_t + 1e-5) / (p_y + 1e-5))\n",
        "            \n",
        "    return mi_xt, mi_ty\n",
        "\n",
        "# get mutual information for all hidden layers\n",
        "def get_mutual_information(hidden):\n",
        "    mi_xt_list = []; mi_ty_list = []\n",
        "    # for hidden in hiddens:\n",
        "    if True:\n",
        "        mi_xt, mi_ty = calc_mutual_information(hidden)\n",
        "        mi_xt_list.append(mi_xt)\n",
        "        mi_ty_list.append(mi_ty)\n",
        "    return mi_xt_list, mi_ty_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u_OKUCe-UwYf",
        "outputId": "a4dbf26c-8077-4b2d-ade7-c2f30e0f38b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_encoder(enc_type='MLP', layer='Linear4'):\n",
        "  \n",
        "    if enc_type == 'MLP':\n",
        "      Net = MLP(dnn_input_units, dnn_hidden_units, dnn_output_units).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(Net.parameters(), lr=3e-3)#, momentum=0.2)\n",
        "    \n",
        "    X_test, Y_test = torch.tensor(x_test, requires_grad=False).float().to(device), torch.tensor(y_test, requires_grad=False).float().to(device)\n",
        "    accuracy_evaluation = {'train': [], 'test': []}\n",
        "    loss_evaluation = {'train': [], 'test': []}\n",
        "    mi_xt_all = []; mi_ty_all = []; epochs = []\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        X_train, Y_train = torch.from_numpy(x_train).float().to(device), torch.from_numpy(y_train).float().to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        out = Net(X_train)\n",
        "        loss = criterion(out, Y_train.argmax(dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % eval_freq == 0 or epoch == num_epochs - 1:\n",
        "            mi_xt, mi_ty = get_mutual_information(Net.models['Linear4'](X_train).cpu().data.numpy())\n",
        "            mi_xt_all.append(mi_xt)\n",
        "            mi_ty_all.append(mi_ty)\n",
        "            print('#'*30)\n",
        "            print('Step - ', epoch)\n",
        "            print('Train: Accuracy - %0.3f, Loss - %0.3f' % (accuracy(out, Y_train), loss))\n",
        "            print('Test: Accuracy - %0.3f, Loss - %0.3f' % (accuracy(Net(X_test), Y_test), criterion(Net(X_test), Y_test.argmax(dim=1))))\n",
        "            print('I(X, T) - ', mi_xt)\n",
        "            print('I(T, Y) - ', mi_ty)\n",
        "            print('Elapse time: ', time.time() - start_time)\n",
        "            print('#'*30,'\\n')\n",
        "\n",
        "    return Net, mi_xt, mi_ty\n",
        "Encoder, true_MI_x, true_MI_y = train_encoder(layer = 'Linear3')\n",
        "print(Encoder)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##############################\n",
            "Step -  0\n",
            "Train: Accuracy - 0.496, Loss - 0.703\n",
            "Test: Accuracy - 0.496, Loss - 0.701\n",
            "I(X, T) -  [1.7959401525079728]\n",
            "I(T, Y) -  [0.008007460799442659]\n",
            "Elapse time:  0.44308924674987793\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  100\n",
            "Train: Accuracy - 0.750, Loss - 0.485\n",
            "Test: Accuracy - 0.751, Loss - 0.483\n",
            "I(X, T) -  [1.9846161172971344]\n",
            "I(T, Y) -  [0.3495621499711857]\n",
            "Elapse time:  1.3837628364562988\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  200\n",
            "Train: Accuracy - 0.873, Loss - 0.211\n",
            "Test: Accuracy - 0.874, Loss - 0.206\n",
            "I(X, T) -  [2.1511290522561697]\n",
            "I(T, Y) -  [0.6931212603355825]\n",
            "Elapse time:  2.3136985301971436\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  300\n",
            "Train: Accuracy - 1.000, Loss - 0.013\n",
            "Test: Accuracy - 1.000, Loss - 0.013\n",
            "I(X, T) -  [2.0620822028026318]\n",
            "I(T, Y) -  [0.693121260335538]\n",
            "Elapse time:  3.1441450119018555\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  400\n",
            "Train: Accuracy - 1.000, Loss - 0.005\n",
            "Test: Accuracy - 1.000, Loss - 0.005\n",
            "I(X, T) -  [2.0559739312808487]\n",
            "I(T, Y) -  [0.6931212603355791]\n",
            "Elapse time:  3.959695339202881\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  500\n",
            "Train: Accuracy - 1.000, Loss - 0.003\n",
            "Test: Accuracy - 1.000, Loss - 0.003\n",
            "I(X, T) -  [1.8591951773955915]\n",
            "I(T, Y) -  [0.693121260335567]\n",
            "Elapse time:  4.776509523391724\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  600\n",
            "Train: Accuracy - 1.000, Loss - 0.002\n",
            "Test: Accuracy - 1.000, Loss - 0.002\n",
            "I(X, T) -  [1.6169308397317417]\n",
            "I(T, Y) -  [0.69312126033554]\n",
            "Elapse time:  5.594787120819092\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  700\n",
            "Train: Accuracy - 1.000, Loss - 0.002\n",
            "Test: Accuracy - 1.000, Loss - 0.002\n",
            "I(X, T) -  [1.601370156913153]\n",
            "I(T, Y) -  [0.6931212603356163]\n",
            "Elapse time:  6.428062677383423\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  800\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.7122161868101877]\n",
            "I(T, Y) -  [0.6931212603355286]\n",
            "Elapse time:  7.245775461196899\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  900\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.7411789401599096]\n",
            "I(T, Y) -  [0.6931212603355587]\n",
            "Elapse time:  8.06488847732544\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1000\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.6450831577302585]\n",
            "I(T, Y) -  [0.6931212603355597]\n",
            "Elapse time:  8.89448356628418\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1100\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.6159392038052338]\n",
            "I(T, Y) -  [0.693121260335565]\n",
            "Elapse time:  9.70537805557251\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1200\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.5236723789239168]\n",
            "I(T, Y) -  [0.6931212603355931]\n",
            "Elapse time:  10.52359127998352\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1300\n",
            "Train: Accuracy - 1.000, Loss - 0.001\n",
            "Test: Accuracy - 1.000, Loss - 0.001\n",
            "I(X, T) -  [1.4602971584956286]\n",
            "I(T, Y) -  [0.6931212603356036]\n",
            "Elapse time:  11.352495908737183\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1400\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.5133959094856215]\n",
            "I(T, Y) -  [0.6931212603356072]\n",
            "Elapse time:  12.163558721542358\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1500\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.5490937079427536]\n",
            "I(T, Y) -  [0.6931212603356101]\n",
            "Elapse time:  12.991466522216797\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1600\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.495155988693814]\n",
            "I(T, Y) -  [0.6931212603356027]\n",
            "Elapse time:  13.819190979003906\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1700\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.4420999715232787]\n",
            "I(T, Y) -  [0.6931212603355987]\n",
            "Elapse time:  14.64417052268982\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1800\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.3895077608935118]\n",
            "I(T, Y) -  [0.6931212603355933]\n",
            "Elapse time:  15.468491554260254\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1900\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.3687280783486704]\n",
            "I(T, Y) -  [0.6931212603355913]\n",
            "Elapse time:  16.29210329055786\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  1999\n",
            "Train: Accuracy - 1.000, Loss - 0.000\n",
            "Test: Accuracy - 1.000, Loss - 0.000\n",
            "I(X, T) -  [1.4833531789107504]\n",
            "I(T, Y) -  [0.6931212603355543]\n",
            "Elapse time:  17.103516817092896\n",
            "############################## \n",
            "\n",
            "MLP(\n",
            "  (full_model): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=16, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=16, out_features=12, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=12, out_features=8, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=8, out_features=6, bias=True)\n",
            "    (7): Tanh()\n",
            "    (8): Linear(in_features=6, out_features=4, bias=True)\n",
            "    (9): Tanh()\n",
            "    (10): Linear(in_features=4, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z3qZ1W8XUwYr",
        "colab": {}
      },
      "source": [
        "from torch.nn.functional import softplus\n",
        "\n",
        "# Neural approximation of MI starts here\n",
        "# Auxiliary network for mutual information estimation\n",
        "class MIEstimator(nn.Module):\n",
        "    def __init__(self, size1, size2):\n",
        "        super(MIEstimator, self).__init__()\n",
        "        \n",
        "        # Vanilla MLP\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(size1 + size2, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "    \n",
        "    # Gradient for JSD mutual information estimation and EB-based estimation\n",
        "    def forward(self, x1, x2):\n",
        "        # breakpoint()\n",
        "        pos = self.net(torch.cat([x1, x2], 1)) #Positive Samples \n",
        "        neg = self.net(torch.cat([torch.roll(x1, 1, 0), x2], 1)) #Predictions for shuffled (negative) samples from p(z1)p(z2)\n",
        "        #breakpoint()\n",
        "        return -softplus(-pos).mean() - softplus(neg).mean(), pos.mean() - neg.exp().mean() + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_FkycCh9UwYw"
      },
      "source": [
        "We are now able to estimate the mutual information while training the network. We'll save the mutual information for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rHCT2r8YuyO",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Subset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class Scheduler:\n",
        "    def __call__(self, **kwargs):\n",
        "        raise NotImplemented()\n",
        "\n",
        "class LinearScheduler(Scheduler):\n",
        "    def __init__(self, start_value, end_value, n_iterations, start_iteration=0):\n",
        "        self.start_value = start_value\n",
        "        self.end_value = end_value\n",
        "        self.n_iterations = n_iterations\n",
        "        self.start_iteration = start_iteration\n",
        "        self.m = (end_value - start_value) / n_iterations\n",
        "\n",
        "    def __call__(self, iteration):\n",
        "        if iteration > self.start_iteration + self.n_iterations:\n",
        "            return self.end_value\n",
        "        elif iteration <= self.start_iteration:\n",
        "            return self.start_value\n",
        "        else:\n",
        "            return (iteration - self.start_iteration) * self.m + self.start_value\n",
        "\n",
        "class ExponentialScheduler(LinearScheduler):\n",
        "    def __init__(self, start_value, end_value, n_iterations, start_iteration=0, base=10):\n",
        "        self.base = base\n",
        "\n",
        "        super(ExponentialScheduler, self).__init__(start_value=math.log(start_value, base),\n",
        "                                                   end_value=math.log(end_value, base),\n",
        "                                                   n_iterations=n_iterations,\n",
        "                                                   start_iteration=start_iteration)\n",
        "\n",
        "    def __call__(self, iteration):\n",
        "        linear_value = super(ExponentialScheduler, self).__call__(iteration)\n",
        "        return self.base ** linear_value\n",
        "\n",
        "def split(dataset, size, split_type):\n",
        "    if split_type == 'Random':\n",
        "        data_split, _ = torch.utils.data.random_split(dataset, [size, len(dataset) - size])\n",
        "    elif split_type == 'Balanced':\n",
        "        class_ids = {}\n",
        "        for idx, (_, y) in enumerate(dataset):\n",
        "            try:\n",
        "                y = int(y.item())\n",
        "            except:\n",
        "                pass\n",
        "            if y not in class_ids:\n",
        "                class_ids[y] = []\n",
        "            class_ids[y].append(idx)\n",
        "\n",
        "        ids_per_class = size // len(class_ids)\n",
        "\n",
        "        selected_ids = []\n",
        "\n",
        "        for ids in class_ids.values():\n",
        "            selected_ids += list(np.random.choice(ids, min(ids_per_class, len(ids)), replace=False))\n",
        "        data_split = torch.utils.data.Subset(dataset, selected_ids)\n",
        "\n",
        "    return data_split\n",
        "\n",
        "\n",
        "class EmbeddedDataset:\n",
        "    BLOCK_SIZE = 1\n",
        "\n",
        "    def __init__(self, base_dataset, encoder, cuda=True):\n",
        "        if cuda:\n",
        "            encoder = encoder.cuda()\n",
        "        self.means, self.target = self._embed(encoder, base_dataset, cuda)\n",
        "\n",
        "    def _embed(self, encoder, dataset, cuda):\n",
        "        encoder.eval()\n",
        "\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.BLOCK_SIZE,\n",
        "            shuffle=False)\n",
        "\n",
        "        ys = []\n",
        "        reps = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in data_loader:\n",
        "                if cuda:\n",
        "                    x = x.cuda()\n",
        "                    y = y.cuda()\n",
        "\n",
        "                p_z_given_x = encoder(x)\n",
        "                reps.append(p_z_given_x.detach())\n",
        "                ys.append(y)\n",
        "\n",
        "            ys = torch.cat(ys, 0)\n",
        "\n",
        "        encoder.train()\n",
        "        return reps, ys\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        y = self.target[index]\n",
        "        x = self.means[index][0]# // self.BLOCK_SIZE][index % self.BLOCK_SIZE]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target.size(0)\n",
        "\n",
        "def train_and_evaluate_linear_model(train_set, test_set, solver='saga', multi_class='multinomial', tol=.1, C=2):\n",
        "    model = LogisticRegression(solver=solver, multi_class=multi_class, tol=tol, C=C)\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    x_train, y_train = build_matrix(train_set)\n",
        "    x_test, y_test = build_matrix(test_set)\n",
        "    x_train = scaler.fit_transform(x_train)\n",
        "    x_test = scaler.transform(x_test)\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    test_accuracy = model.score(x_test, y_test)\n",
        "    train_accuracy = model.score(x_train, y_train)\n",
        "\n",
        "    return train_accuracy, test_accuracy\n",
        "\n",
        "\n",
        "def build_matrix(dataset):\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for x, y in data_loader:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    xs = torch.cat(xs, 0)\n",
        "    ys = torch.cat(ys, 0)\n",
        "\n",
        "    if xs.is_cuda:\n",
        "        xs = xs.cpu()\n",
        "    if ys.is_cuda:\n",
        "        ys = ys.cpu()\n",
        "\n",
        "    return xs.data.numpy(), ys.data.numpy()\n",
        "\n",
        "def evaluate(encoder, train_on, test_on, cuda):\n",
        "    embedded_train = EmbeddedDataset(train_on, encoder, cuda=cuda)\n",
        "    embedded_test = EmbeddedDataset(test_on, encoder, cuda=cuda)\n",
        "    return train_and_evaluate_linear_model(embedded_train, embedded_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91duTE0CUrud",
        "colab_type": "code",
        "outputId": "6b5df881-ec11-4d78-aaed-1f6448fd92e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_MI(encoder, num_epochs=2000, dnn_hidden_units=[]):\n",
        "    \n",
        "    #x_train, y_train, x_train_int = generate_samples(n_train_samples) # training dataset\n",
        "    #x_test, y_test, _ = generate_samples(n_test_samples) # testing dataset\n",
        "    mi_xt_all = []; mi_ty_all = []; epochs = []\n",
        "    mi_xz = []; mi_zy = []\n",
        "\n",
        "    X_test, Y_test = torch.tensor(x_test, requires_grad=False).to(device), torch.tensor(y_test, requires_grad=False).to(device)\n",
        "\n",
        "    z_test = torch.tensor(encoder(X_test.float()), requires_grad=False).to(device)\n",
        "\n",
        "    x_dim, y_dim, z_dim = x_test.shape[-1], y_test.shape[-1], z_test.shape[-1]\n",
        "    print(x_dim, y_dim, z_dim)\n",
        "    mi_estimator_X = MIEstimator(x_dim, z_dim).to(device)\n",
        "    mi_estimator_Y = MIEstimator(z_dim, y_dim).to(device)\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "    {'params': mi_estimator_X.parameters(), 'lr':1e-3},\n",
        "    {'params': mi_estimator_Y.parameters(), 'lr':1e-3},\n",
        "    ])\n",
        "\n",
        "    beta_scheduler = ExponentialScheduler(start_value=1e-9, end_value=1e-2, n_iterations=num_epochs, start_iteration=10)\n",
        "\n",
        "    accuracy_evaluation = {'train': [], 'test': []}\n",
        "    loss_evaluation = {'train': [], 'test': []}\n",
        "\n",
        "    start_time = time.time()\n",
        "    max_MI_x, max_MI_y = 0, 0\n",
        "    mi_est_all = {'X': [], 'Y': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        beta = beta_scheduler(epoch)\n",
        "\n",
        "        X_train, Y_train = torch.from_numpy(x_train).to(device).float(), torch.from_numpy(y_train).to(device).float()\n",
        "        z_train = encoder(X_train.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        mi_gradient_X, mi_estimation_X = mi_estimator_X(X_train, z_train)\n",
        "        mi_gradient_X = mi_gradient_X.mean()\n",
        "        mi_estimation_X = mi_estimation_X.mean()\n",
        "\n",
        "        mi_gradient_Y, mi_estimation_Y = mi_estimator_Y(z_train, Y_train.float())\n",
        "        mi_gradient_Y = mi_gradient_Y.mean()\n",
        "        mi_estimation_Y = mi_estimation_Y.mean()\n",
        "                \n",
        "        loss_mi = - mi_gradient_Y - mi_gradient_X\n",
        "        loss_mi.backward()\n",
        "        optimizer.step()\n",
        "        mi_est_all['X'].append(mi_estimation_X.cpu().data.numpy())\n",
        "        mi_est_all['Y'].append(mi_estimation_Y.cpu().data.numpy())\n",
        "        eval_freq = 10\n",
        "        if epoch >= 500:\n",
        "          if mi_estimation_X > max_MI_x:\n",
        "            max_MI_x = mi_estimation_X.cpu().data.numpy()\n",
        "          if np.mean(mi_est_all['X'][-100]) > max_MI_x - 1e-1:\n",
        "            break\n",
        "        if epoch % eval_freq == 0 or epoch == num_epochs - 1:\n",
        "            mi_xt, mi_ty = get_mutual_information(z_train.cpu().data.numpy())\n",
        "            mi_xt_all.append(mi_xt)\n",
        "            mi_ty_all.append(mi_ty)\n",
        "            print('#'*30)\n",
        "            print('Step - ', epoch)\n",
        "            # print('Train: Accuracy - %0.3f, Loss - %0.3f' % (accuracy(out, Y_train), loss))\n",
        "            # print('Test: Accuracy - %0.3f, Loss - %0.3f' % (accuracy(predictor(z_test.float()), Y_test), criterion(predictor(z_test.float()), Y_test.argmax(dim=1))))\n",
        "            print('I(X, T) - ', mi_xt)\n",
        "            print('I(T, Y) - ', mi_ty)\n",
        "            print('I_est(X, T)', mi_estimation_X)\n",
        "            #print('Grad I_est(X, T)', mi_gradient_X)\n",
        "            print('I_est(T, Y)', mi_estimation_Y)\n",
        "            #print('Grad I_est(T, Y)', mi_gradient_Y)\n",
        "            print('Time after last eval: ', time.time() - start_time)\n",
        "            print('#'*30,'\\n')\n",
        "            \n",
        "    return max_MI_x, max_MI_y\n",
        "\n",
        "n_train_samples = 50000\n",
        "n_test_samples = 10000\n",
        "\n",
        "\n",
        "num_labels_range = [2**i for i in range(1, int(np.log2(n_train_samples)-8))]\n",
        "\n",
        "acc = {i:[] for i in num_labels_range}\n",
        "print(num_labels_range)\n",
        "seeds = [9, 42, 103, 48, 79]\n",
        "layers = ['Linear2', 'Linear 3', 'Linear4']\n",
        "mi_layers = {}\n",
        "for seed_ in seeds:\n",
        "\n",
        "    \n",
        "    print('\\nRunning with seed ', seed_)\n",
        "    torch.manual_seed(seed_)\n",
        "    np.random.seed(seed_)\n",
        "    seed(seed_)\n",
        "    \n",
        "    \n",
        "    for layer in layers:\n",
        "\n",
        "        x_train, y_train, x_train_int = generate_samples(n_train_samples) # training dataset\n",
        "        x_test, y_test, _ = generate_samples(10*n_test_samples) # testing dataset\n",
        "        if seed_ == 9:\n",
        "            MI_X, MI_Y = train_MI(Encoder.models[layer], num_epochs=2000)\n",
        "            mi_layers[layer] = (MI_X, MI_Y)\n",
        "        y_train = y_train.argmax(axis=1)\n",
        "        y_test = y_test.argmax(axis=1)\n",
        "        train_set = torch.utils.data.TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device)) \n",
        "        test_set = torch.utils.data.TensorDataset(torch.Tensor(x_test).to(device), torch.Tensor(y_test).to(device))\n",
        "    \n",
        "        print('MI values for %s - %s, %s' % (layer, MI_X, MI_Y))\n",
        "        for num_labels in num_labels_range:\n",
        "            print('Evaluating for %s %s' % (layer, num_labels))\n",
        "            #acc[layer].append(train_MI(num_labels, Encoder, num_epochs=2000))\n",
        "            \n",
        "\n",
        "\n",
        "            train_subset = split(train_set, num_labels, 'Balanced') \n",
        "            test_subset = split(train_set, n_test_samples, 'Random') \n",
        "            train_accuracy, test_accuracy = evaluate(encoder=Encoder.models[layer], train_on=train_subset, test_on=test_subset, cuda=torch.cuda.is_available())\n",
        "            #train_accuracy, test_accuracy = train_and_evaluate_linear_model(train_subset, test_set, C=2)\n",
        "            print('Train Accuracy: %f'% train_accuracy)\n",
        "            print('Test Accuracy: %f'% test_accuracy)\n",
        "            acc[num_labels].append(test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 4, 8, 16, 32, 64]\n",
            "\n",
            "Running with seed  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10 2 8\n",
            "##############################\n",
            "Step -  0\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(-0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(-0.0058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  0.6959233283996582\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  10\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(0.1403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.4235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  3.1214561462402344\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  20\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(0.4866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  5.543750762939453\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  30\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(0.7446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  7.99549674987793\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  40\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  10.442768096923828\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  50\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.3780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  12.87470293045044\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  60\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.5697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  15.289467334747314\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  70\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.6998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  17.710773706436157\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  80\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.7946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  20.200495958328247\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  90\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.8617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  22.641329050064087\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  100\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.9142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  25.05216956138611\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  110\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(1.9710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  27.46847701072693\n",
            "############################## \n",
            "\n",
            "##############################\n",
            "Step -  120\n",
            "I(X, T) -  [3.48226004319869]\n",
            "I(T, Y) -  [0.6931421877511955]\n",
            "I_est(X, T) tensor(2.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "I_est(T, Y) tensor(0.6977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Time after last eval:  29.916054248809814\n",
            "############################## \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqS3_Be1nDrG",
        "colab_type": "code",
        "outputId": "c1207293-d4e9-457a-9c97-7e0514f3dca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "values = []\n",
        "for key in acc.keys():\n",
        "    values.append((np.mean(acc[key]), np.std(acc[key])))\n",
        "means = np.array([i[0] for i in values])\n",
        "stds = np.array([i[1] for i in values])\n",
        "fig, ax = plt.subplots(1, 1, sharex=True)\n",
        "nums = np.array(num_labels_range)\n",
        "ax.plot(nums/2, means, color='black', label='Linear-2')\n",
        "ax.fill_between(nums/2, means - stds, means + stds, facecolor='yellow', interpolate=True)\n",
        "ax.set_xlabel('Examples per label')\n",
        "ax.set_ylabel('Accuracy')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5wcVZn/8c+TyZWQK7kQcgcSSYKA\nMBsWlnWRiwLyI14AiYsrLi6yC4go/gQXSYgGUFkvKywY2XBb5SIoG3+wZhHxxerqbgYEMkkkhhDJ\nhEsCJEwukAvz/P44NUzPdM+kp7urq6vr+3696tVd1VXVT6Un5+k65/Q55u6IiEh29Uk6ABERSZYS\ngYhIxikRiIhknBKBiEjGKRGIiGRc36QD6K1Ro0b5lClTkg5DRCRVnnjiiVfdfXSh11KXCKZMmUJT\nU1PSYYiIpIqZ/am711Q1JCKScUoEIiIZp0QgIpJxSgQiIhmnRCAiknFKBCIiGadEICKScUoEIiIZ\nl7oflEklrQMuBiYAE6NlQs7joMQiE5HqUSLItOuBh3p4fT86J4dCzwfEHKOIxE2JILM2ALfvZZ/X\nouWpHvYZTffJYkK09C8zVhGJkxJBZn0T2FmB82yKlie7ed2AMfR8VzEe6FeBWESkFEoEmbQR+EGV\n3suBV6LliW726QOMpfOdRNekcQD6cxWJh/5nZdK3gB1JB5GjDXgpWpZ1s08fYH+6v6uYCIwDGuIO\nVqTuKBFkzmbgX5IOogRtwIvR8j/d7NNASAaF2irat+2Pek2LdKZEkDnfBbYmHURM3gZaouW33ezT\nl1DN1FNvqLGEtg2RbFAiyJStwD8nHUTC9gAvREt3+hEasHtq4B6DkoXUCyWCTLmJUDUkPdtN+LHd\nuh726QcMA4bmPA4tYr3rc3WtleQpEWTGDkIjsVTGbuDVaCnHAHqfQAq9pu63Ujolgsz4PqG/v9SW\nnXT8FqMcA6lMQlGRkEX61DNhJ3BD0kFIrN6Klo1lnmcQlUko6sabJkoEmbCY0O1SZG/ejJZXyjzP\nPpSfUIaihFIdSgR1bw/w9aSDkMzZES0vl3mewVQmoei3Iz2JLRGY2WLgdGCjux9a4HUjdGo/jfAX\nc567dzdgjZTsLuBPSQchUqLt0fJSGecwKpNQhlCvCSXOO4LbgRuBO7t5/VRgWrQcDdwcPUrFtAHX\nJR2ESMIc2BYt5VSRGrAvlUkotfUblNgSgbs/bmZTethlDnCnuzvwOzMbbmbj3L2c1C+d3Av8Mekg\nROqEE36UuZUwjHvx2tpg61ZobYXWVuONNwbR2roPra0DaG0dwBtv9KO1tSF6DVpb22htfZvW1j28\n8cZOWlt30tr6Ft/5znc4//zzK35lSbYRjAfW56y3RNvyEoGZXQBcADBp0qSqBJd+DlybdBAiqdbW\nBtu2tRfgYQkFdf7znl7b2mlUF6ejDaWDGQwZAkOH9mPo0EEMGzaM4cP3Y9Kk/Rk2bDxDhw5j5syZ\nsVxnKhqL3X0RsAigsbHREw4nJR4EmpMOQiQR7pUrwL2IEicU4B3LsGEwYULH866vDR3al6FDJzFs\n2HSGDp3F0KFHsO++R9Knz3SSKJaTTAQbCAO3tJtAb++3pAdfSzoAkV5zhx07Siu0c9e3bg3f5vdm\n8ODOBfWwYXDAAV0L7Z4KdNh3X2jotpdrP2A6MCtnmUloGq2d7+FJRrIEuNjM7iE0Er+h9oFKeZju\nZwwTqTx3ePPN0grtrs+LKcD32Se/YN5//70X2rnPhwzpqQDvrX6Ewj23sJ8Vbav94T/i7D56N3A8\nMMrMWoB5RP8i7n4LobQ6DVhDqCz7VFyxZI/uBqQ47vDWW6UX2rnrb7+99/cbNCi/YD7ooOK+ebc/\nHzIE+ib2FbYvoXBvL+jbl+mkocDvTpy9hubu5XUHLorr/bPrl3Q/Fr/UC3fYubP0Qjv3+Z49e3+/\ngQPzC+apU4v75p279EtNWdkXOJjCBX79jRhbO5VUUiG6G6h17QV4ud/Cd+/e+3sNGJBfME+eXHz9\nd/vSv/7KvkgDcBCdC/tZwLuoxwK/O0oEdeW/gceSDqKuucNrr8GGDbBlS2kF+q5de3+ffv3yC+qJ\nE/f+zbtrAT5gQPz/JunQh+4LfP0jKRHUFd0NlMMdNm+G9evD0tKS/7ylJdSpd6dv3/zCePx4mDGj\n+EbM9gLcauvHpynRBziQwgX+wATjqm1KBHXjSeA/kg6iZrmHb/DdFe7tjzs6/8aHhobQnXDiRDjq\nKJgzJzwfPx5GjswvxAcOVAFeHX2AqeQX+IegAr/3lAjqRrbvBt54o3Dhnvt8+/bOx/TpEwr5CRPg\n8MPh9NPD84kTOx7337+SXQyl94z8An8mMIMwd4JUghJBXWgm/JK4Pm3d2n3h3v6880/4w7fyceNC\nYT5rFpxySkfh3l7QjxuXZDdE6cyAKXTugz+LUODvk1xYGaH/BnVhIWH8kvTZvn3vdfJvvNH5GDMY\nOzYU6IccAief3Plb/MSJoZBPT1fFLDFgMoUL/MEJxpVtSgSptxq4L+kgCtqxo+dv8evXh3r7rsaO\nDYX6tGnwvvd1/hY/cWKozqnf7oz1woBJ5PfDn0EYyllqiRJB6l1HmHegut58MxTmPdXJv/56/nGj\nR4cCfepUeO9786trxo9Xl8f0mUh+o+1MVOCnhxJBqv0J+LeKn3Xnzs6FfKGC/tVX84/bb7+OQv3Y\nY/Ora8aPD71qJK0mULjAH5JkUFIBSgSpdj1hTuLSrFkDDzyQX9Bv2pS/74gRHQX60UfnV9eMHx8G\nApN6MJ7CBf7QJIOSGCkRpNaLwG0lH71rF5x4IrzwAgwf3lGgNzbmd6GcMCEM1yv15gDyC/tZhGkV\nJUuUCFLrm8DOko9evDgkgYcegtNOq1xUUovGkV/YzwKGJxmU1BAlglTaRDRhW0neegsWLgz1+Kee\nWrmoJGn7k1/YzwJGJBmUpIASQSp9i67znfbGrbeGNoHbb9dwCOk0lsIF/sgkg5IUUyJInc3ATSUf\n/eabcN118Jd/CSecULmoJA5jyC/sZwH7JRmU1CElgtT5LrB1r3t1Z9EiePFF+OEPdTdQO0aRX9jP\niraLxE+JIFW2Av9c8tE7doS7gfe9D44/vmJBSdH2o3C3zDFJBiWiRJAuNxGqhkpz883wyivw4x9X\nLiIpZCSFC/yxSQYl0i0lgtTYQWgkLs327fD1r8NJJ4X2AamEERTuh79/kkGJ9JoSQWosInQbLc1N\nN4VfDF9zTeUiyqajgbOBjxCGTRZJPyWCVNgJ3FDy0Vu3wje+EcbkP/bYykWVHbOBs6JlcsKxiFSe\nEkEq3AZsKPno730vTLiuu4HeaCR88z8LffOXeqdEUPP2AF8v+ejWVrjhBvjgB2H27MpFVZ+OJBT+\nZxOmRxTJBiWCmncXsK7ko7/7Xdi8WXcD3XsP4Vv/2cBBCccikow+cZ7czE4xs2fNbI2ZXVHg9clm\n9qiZPWNmvzKzCXHGkz5thIlnSrNlC/zTP8GcOXDUUZWLKv0OJ0zv+UfgSeBKlAQky2K7IzCzBkLH\n95OBFmCZmS1x95U5u90A3Onud5jZCYRS7xNxxZQ+9xIKq9J8+9thvt/58ysWUIq9m45qn+kJxyJS\nW+KsGpoNrHH3tQBmdg8wB8hNBDOBz0fPHwMejDGelHHg2pKPfv11+M534CMfgSOOqFxU6XIoHdU+\nhyQci0jtirNqaDywPme9JdqW62lCh2yADwNDzCxvRC0zu8DMmsysaVOh6bPq0oNAc8lHf+tboaE4\ne3cDM4H5hO8by4GrURIQ6VmsbQRFuBz4KzP7PfBXhD6Sb3fdyd0XuXujuzeOHj262jEmZGHJR776\namgkPvtsePe7KxhSzTqEUOA3AyuAecCMRCMSSZM4q4Y2ABNz1ifQpTO8u79IdEdgZvsCH3X3LTHG\nlBL/ATxR8tE33BCGlJg3r3IR1Z7pdNT5ZyLbicQmzkSwDJhmZlMJCeAc4OO5O5jZKOB1d28jdN1Y\nHGM8KfK1ko/cuBFuvBHOOQdmzqxgSDVhGh11/ocnHItI/YgtEbj7HjO7GFgKNACL3X2FmS0Amtx9\nCXA8cJ2ZOfA4cFFc8aTHY8B/l3z0N78ZJp+5+urKRZSsg+j45p/ZVm+RWJm7Jx1DrzQ2NnpTU1PS\nYcToROCXJR358stw4IFw5plw552Vjaq6DqTjm/+RCcciUh/M7Al3byz0mn5ZXFP+m1KTAIRhpnft\ngq98pXIRVc8UOgr/gn+rIhITJYKaUnrbwIsvwi23wCc+AdOmVTCkWE2mo/D/s4RjEckuJYKa8SSh\nt1Bprr8edu9Ow93ARDoK/6MTjkVEQImghpR+N9DSAt//PnzqU6GNoPZMAM4kFP5/Dliy4YhIJ0oE\nNWEF5Yyuce210NYG//iPlYuofOPpKPyPQYW/SO1SIqgJCwljC/XeCy/ArbfC+efDlCkVDaoE4+go\n/P8CFf4i6aBEkLg/AveVfPTChWAGX/5y5SLqnf2BjxIK/+NIftQSEektJYLEXUeB4ZWK8vzzsHgx\nfOYzMGlSZaPq2VhC4X8W8F5U+IukmxJBov4E/FvJR3/ta9DQAFdeWbmIujeGMCzU2YTxAVX4i9QL\nJYJEXQ/sLunI556DO+6Aiy6C8V0H966YUXQU/scTRgoRkXqjRJCYF4HbSj76q1+Ffv3girwJQMu1\nH6HwPws4ARX+IvVPiSAx3wR2lnTk6tVw111w6aUwblwlYhlJmBfobELhrz8LkSzR//hEbAIWlXz0\nggUwYAB86UvlxjES+CFwEvpTEMkutfgl4lvAjpKOXLUK7r4bLr4Yxo4tN44rgFNQEhDJNiWCqtsM\n3FTy0QsWwKBB8MUvlhvHeOCSck8iInVAiaDq/hnYWtKRzc1w771wySVQ/tTN84CB5Z5EROqAEkFV\nbSUkgtJccw0MHgyXX15uHNOBvy33JCJSJ5QIqupfgNdLOvKZZ+D+++Fzn4P99is3jq+ibqEi0k6J\noGp2EBqJSzN/PgwdCp//fLlxHEX4jYCISKBEUDWLgI0lHfnkk/DTn8Jll8GIEeXGcS0aFVREcikR\nVMVO4IaSj54/H4YPD9VC5TkBeH+5JxGROrPXRGBml5hZ2d9Ds+02YENJRzY1wc9+Bl/4QkgG5bmu\n3BOISB0q5pdEY4FlZvYksBhY6u6lzaKSGo9GjwOiZWDO867re8ule4CvlxzJvHkwciR89rMlnyLy\nYWB2uScRkTq010Tg7leZ2VcIdQqfAm40s/uAf3X35+IOsPp2Ah+g+DkC+tJzstgNrCspkt/9Dh5+\nOExFOXRoSaeINBBmQRMRyVfU2ALu7mb2MvAy4SvuCOB+M3vE3f9vnAFW32p6N1HMnmjZXvFI5s+H\nUaPCcBLl+RtgRvkBiUhd2msiMLNLCSXJq8CtwBfdfbeZ9SHMs1hniWBV0gEA8JvfwNKl8I1vwJAh\n5ZxpAHBNhaISkXpUTK+hkcBH3P0D7v5jd98N4O5twOk9HWhmp5jZs2a2xszyRs43s0lm9piZ/d7M\nnjGz00q6ioqqjUQwbx6MGQP/8A/lnukfgIkViEhE6lUxieA/yPk5rJkNNbOjAdy921LTzBoIo6ud\nCswE5prZzC67XQXc5+7vAc4h/PQ2Yckngscfh0cfDcNMDx5czpmGAInNai8iKVFMIrgZ2Jazvi3a\ntjezgTXuvtbddwH3AHO67ONAezPoMMK0XQlLPhHMmwf77w8XXljumb5AmG5SRKR7xTQWW253UXdv\nM7NijhsPrM9ZbwGO7rLPfOA/zewSYDBhhpT8AMwuAC4AmDRpUhFvXao2QmNxch57DH71K/jud2Gf\nfco502hCIhAR6VkxdwRrzeyzZtYvWi4F1lbo/ecCt7v7BOA04K6oEboTd1/k7o3u3ji6/PGXe/A8\n8FaM5++ZO1x9NRxwAFxwQbln+0dg3wpEJSL1rphEcCFwLOGnse3f6osppjbQuZVyAvk/rz0fuA/A\n3X9L6HyfYF3GH5J7a+AXv4Bf/xq+/GUYWNZUAZOBv69QVCJS74r5QdlGQkNuby0DppnZVEICOAf4\neJd9XgBOBG43sxmERLCphPeqkOTaB9xD28CECfDpT5d7tmuA/hWISkSyoJjfEQwkfHOfRc6UVu7e\n48wm7r7HzC4GlhJ+2rrY3VeY2QKgyd2XECqxf2BmlxEajs9LdviK5BLB0qXw29/CLbeEielLNwv4\nRIWiEpEsKKbR9y5CnckHgAXAX1NkienuDwMPd9l2dc7zlcBfFBts/JJJBO1tA5Mnw6c+Ve7ZFqJB\nZUWkN4opMQ52968A2939DuCD5Pf+qRPJJIKHHoJly+Cqq6B/WTU6x5DfQ1dEpGfFJILd0eMWMzuU\n0N9/THwhJeVlYEvV37W9bWDqVPjkJ8s92/WVCElEMqaYqqFF0XwEVwFLCH0SvxJrVIlI5m5gyZIw\nA9ltt0G/fuWc6RTgvRWKSkSypMdEEPXpb3X3zcDjwIFViSoR1U8EbW3hbuDgg+Hcc8s5kxGmoBQR\n6b0eq4aigeXqbHTR7lQ/Efz0p/D006GhuG9RA4J352zgPRWKSkSyppg2gl+Y2eVmNtHMRrYvsUdW\nddVNBG1tYb6Bd70L5s4t50x9ga9VJigRyaRivod+LHq8KGebU3fVRNVNBPffD83N8KMflXs3cD5w\ncIWiEpEssrRNP9zY2OhNTU0VPmsroTNUdbz9Nrz73eH58uXQ0FDqmQYBzwHjKhOYiNQtM3vC3RsL\nvVbML4v/ptB2d7+z3MBqR3XvBu69F1atCo+lJwGAz6IkICLlKqZS4s9yng8kjA30JKBEUII9e+Ca\na+DQQ+HMM8s503DgSxWKSkSyrJhB5y7JXTez4YRJZupI9RLB3XfD6tXwwAPQp6yRIL4EjKhQVCKS\nZaUURduBqZUOJFnVSQTtdwOHHw4f+lA5ZzoAuLRCUYlI1hXTRvAzQi8hCIljJtEcAvWjOongrrvg\nuefgwQfLvRv4CqGhWESkfMW0EdyQ83wP8Cd3b4kpngTsJMxMFq/du+GrX4Ujj4QzzijnTAcDZU9Y\nICLyjmISwQvAS+7+FoCZDTKzKe6+LtbIqmY18Hbs73LHHfD88/C974FZOWdaQHEfm4hIcYqpoPgx\nYVb3dm9H2+pE/NVCu3aFu4HZs+G008o50xGUNlmciEj3ivlq2dfdd7WvuPsuM6ujeRDjTwSLF8ML\nL8D3v1/u3cC1hAHmREQqp5g7gk1m9k6ttpnNAV6NL6RqizcR7NwJCxfCMcfABz5Qzpn+Cji1QlGJ\niHQo5o7gQuCHZnZjtN4CFPy1cTrFmwhuvRVaWsJdQXl3A9dVKiQRkU6K+UHZc8Cfm9m+0fq22KOq\nmjZCY3GHVavgJz8J9fq7doXePu3PC63vbdvGjXDccXDSSeXEeQZhGkoRkcor5ncE1wLfcPct0foI\n4AvuflXcwcVvHfBWpy1XXgn//u/heb9+Yenfv2Pput6+bfBgGDEif79Bg+Cyy8q5G+iDJp0RkTgV\nUzV0qrt/uX3F3Teb2WmEqStTLr9aaPnyMAbQffeVW5VTKecCs5IOQkTqWDGNxQ1mNqB9xcwGAQN6\n2D9FOieCbdtg7Vo47LBaSQL9Cb8bEBGJTzF3BD8EHjWz2wh9F88D7ogzqOrpnAhWrgyPhx6aQCgF\nfQaYnHQQIlLnimks/rqZPQ2cRBhzaCl1Uzp1TgTNzeGxfdKYZO1LXdS+iUjNK3bos1cISeAs4ASK\n7HNpZqeY2bNmtsbMrijw+rfN7KloWW1mW4qOvCI6X8by5aFxd2pNjK16GTAm6SBEJAO6vSMws+nA\n3Gh5FbiXMLXl+4o5sZk1ADcBJxN+e7DMzJa4+8r2fdz9spz9LwHeU8pFlOZloHPeaW6GmTPLnTWs\nEkYBlycdhIhkRE93BH8gfPs/3d2Pc/fv0bvR2WYDa9x9bTRExT3AnB72nwvc3Yvzlyn/pqa5uVaq\nha4EhiYdhIhkRE+J4CPAS8BjZvYDMzuR3g10Mx5Yn7PeEm3LY2aTCZPd/LIX5y9T50Tw6qvw8su1\n0FA8Ebgo6SBEJEO6TQTu/qC7nwMcAjwGfA4YY2Y3m9n7KxzHOcD97l7wjsPMLjCzJjNr2rRpU4Xe\nsnBDcfKJYD510ztXRFJhr43F7r7d3X/k7v8HmAD8nuJmTd9A+HrbbkK0rZBz6KFayN0XuXujuzeO\nHj26iLcuRi32GJoBfDLJAEQkg3o1YaK7b44K5ROL2H0ZMM3MpkbDVp8DLOm6k5kdQpiF/be9iaV8\n+YlgxAgYN666UXT2NSDxlmoRyZiyZs7tibvvAS4m/O5gFXCfu68wswW5w1oTEsQ97u6FzhOPVuDF\nTluWLw/VQsn9ong2oVlGRKS6Yp3z0N0fBh7usu3qLuvz44yhsM53A+7hjuDcc6sfSQcNMy0iyYjt\njqC2dU4ELS3Q2ppkQ/FJhJ66IiLVp0RAqBaCpBKBobsBEUmSEgFJdx39KNCYxBuLiABKBEBIBOPH\nh15D1dUXWFjtNxUR6SSDiWAn8HynLe09hqrvPGB6Em8sIvKODCaC1eQOmbRnT5inuPo/JBtI+BWx\niEiyMpgIOlcLPfcc7NyZxB3BxXQz9JKISFVlMBH8odNaMj2GhhFGGBURSV4GE0F+Q7FZmIeger4I\njKzmG4qIdEuJoBkOPjjMTFYdYwkDuYqI1IaMJYI24NlOW6rfY+gqYHA131BEpEcZSwTrgLfeWXvz\nTVizppo9hqYCn6nWm4mIFCVjiaBztdAf/gBtbdW8I1gA9KvWm4mIFCXTiaC6Q0scBny8Gm8kItIr\nmU4Ey5dD//4wbVo13nshmfvnFpFUyFjJ9EqnteZmmDED+sY6KwPAccDpcb+JiEhJMpYIOk+C1txc\nrWohDTMtIrUr9u/CtWrLFli/vlI9hgYD+0XLyJzn+xF6Ch1XiTcREYlFZhPBihXhsfAdwbuAMRQu\n2AutD4g7XBGR2GQ2EXTfY2gAsBx18xSRrMhYG0GH5cthyBCYNKnrK7NQEhCRLMlsImhvKDbr+soR\nSYQjIpKYTCYC9556DCkRiEi2ZDIRvPwyvPZad4ngPdUOR0QkUZlMBO0NxfldRw04vMrRiIgkK9OJ\nIP+O4EBgSJWjERFJVqyJwMxOMbNnzWyNmV3RzT5nm9lKM1thZj+KM552y5fDmDEwenTXV9Q+ICLZ\nE9vvCMysAbgJOBloAZaZ2RJ3X5mzzzTC5L1/4e6bzWxMXPHkam7u7hfFah8QkeyJ845gNrDG3de6\n+y7gHmBOl33+DrjJ3TcDuPvGGOMBwvwDK1aox5CISLs4E8F4YH3Oeku0Ldd0YLqZ/cbMfmdmpxQ6\nkZldYGZNZta0adOmsoJatw527FAiEBFpl3RjcV9gGnA8MBf4gZkN77qTuy9y90Z3bxydX7HfK8uX\nh8f8qqHR5OcpEZH6F2ci2ABMzFmfEG3L1QIscffd7v48sJqQGGLT3mNo5syur+huQESyKc5EsAyY\nZmZTzaw/cA6wpMs+DxLuBjCzUYSqorUxxkRzM0yZEsYZ6kyJQESyKbZE4O57gIuBpYQ5Iu9z9xVm\ntsDMzoh2Wwq8ZmYrgceAL7r7a3HFBKFqSD2GREQ6xDoMtbs/DDzcZdvVOc8d+Hy0xG7XrjaefRbO\nOKPQq7ojEJFsSrqxuKpWr97Onj2FegztQ5iMRkQkezKVCJYv3woUqho6lIz9U4iIvCNTpV9z81b6\n9oV35X35V/uAiGRX5hLB9OnQv3/XV9Q+ICLZlalEsHz51m56DCkRiEh2ZSYRbNu2jeeff7NAQ3Ef\n4LAEIhIRqQ2ZSQQrV4ZBT/MTwXRCryERkWzKTCJojsaWyK8aUrWQiGRbZhLBoEGDOPbYEUyd2vUV\nJQIRybbMJIK5c+fym98cQ5+8K1bXURHJtswkgu7pjkBEsi3jiWAcUJXZMUVEalbGE4GqhUREMp4I\nVC0kIqJEICKScRlPBKoaEhHJcCIYAhyUdBAiIonLcCI4DLCkgxARSVyGE4HaB0REINOJQO0DIiKQ\n6USgOwIREchsIuhLmKdYREQymghmAAOSDkJEpCZkNBGoWkhEpJ0SgYhIxmU0EajHkIhIu1gTgZmd\nYmbPmtkaM7uiwOvnmdkmM3sqWj4dZzwddEcgItKub1wnNrMG4CbgZKAFWGZmS9x9ZZdd73X3i+OK\nI98kYET13k5EpMbFeUcwG1jj7mvdfRdwDzAnxvcrku4GRERyxZkIxgPrc9Zbom1dfdTMnjGz+81s\nYqETmdkFZtZkZk2bNm0qMyy1D4iI5Eq6sfhnwBR3Pwx4BLij0E7uvsjdG929cfTo0WW+pe4IRERy\nxZkINgC53/AnRNve4e6vufvOaPVW4KgY44koEYiI5IozESwDppnZVDPrD5wDLMndwczG5ayeAayK\nMR5gJDAl3rcQEUmZ2HoNufseM7sYWAo0AIvdfYWZLQCa3H0J8FkzOwPYA7wOnBdXPMHh8Z5eRCSF\nzN2TjqFXGhsbvampqcSjfwGcVMlwRERSwcyecPfGQq8l3VhcZWofEBHpKmOJYFTSAYiI1JyMJQIR\nEelKiUBEJOOUCEREMk6JQEQk45QIREQyTolARCTjlAhERDJOiUBEJOOUCEREMi51Yw2Z2SbgT102\njwJeTSCcSquH69A11AZdQ22opWuY7O4FJ3RJXSIoxMyauhtMKU3q4Tp0DbVB11Ab0nINqhoSEck4\nJQIRkYyrl0SwKOkAKqQerkPXUBt0DbUhFddQF20EIiJSunq5IxARkRIpEYiIZFzqE4GZnWJmz5rZ\nGjO7Iul4SmFm68xsuZk9ZWalTshcVWa22Mw2mllzzraRZvaImf0xehyRZIx70801zDezDdFn8ZSZ\nnZZkjHtjZhPN7DEzW2lmK8zs0mh7aj6LHq4hbZ/FQDP7XzN7OrqOa6LtU83sf6Iy6l4z6590rF2l\nuo3AzBqA1cDJQAuwDJjr7isTDayXzGwd0OjutfLDk70ys/cC24A73f3QaNs3gNfd/fooKY9w9y8l\nGWdPurmG+cA2d78hydiKZWbjgHHu/qSZDQGeAD4EnEdKPoseruFs0vVZGDDY3beZWT/g18ClwOeB\nn7j7PWZ2C/C0u9+cZKxdpWfjZ4oAAAYRSURBVP2OYDawxt3Xuvsu4B5gTsIxZYK7Pw683mXzHOCO\n6PkdhP/MNauba0gVd3/J3Z+Mnm8FVgHjSdFn0cM1pIoH26LVftHiwAnA/dH2mvws0p4IxgPrc9Zb\nSOEfEOGP5T/N7AkzuyDpYMow1t1fip6/DIxNMpgyXGxmz0RVRzVbpdKVmU0B3gP8Dyn9LLpcA6Ts\nszCzBjN7CtgIPAI8B2xx9z3RLjVZRqU9EdSL49z9SOBU4KKoyiLVPNQ5prHe8WbgIOAI4CXgn5IN\npzhmti/wAPA5d2/NfS0tn0WBa0jdZ+Hub7v7EcAEQo3FIQmHVJS0J4INwMSc9QnRtlRx9w3R40bg\np4Q/oDR6Jarvba/33ZhwPL3m7q9E/5nbgB+Qgs8iqo9+APihu/8k2pyqz6LQNaTxs2jn7luAx4Bj\ngOFm1jd6qSbLqLQngmXAtKhVvj9wDrAk4Zh6xcwGRw1kmNlg4P1Ac89H1awlwCej558E/j3BWErS\nXnhGPkyNfxZRA+W/Aqvc/Vs5L6Xms+juGlL4WYw2s+HR80GETiyrCAnhzGi3mvwsUt1rCCDqUvYd\noAFY7O4LEw6pV8zsQMJdAEBf4EdpuAYzuxs4njDM7ivAPOBB4D5gEmGo8LPdvWYbY7u5huMJVREO\nrAM+k1PXXnPM7Djgv4DlQFu0+cuEOvZUfBY9XMNc0vVZHEZoDG4gfMm+z90XRP/H7wFGAr8HznX3\nnclFmi/1iUBERMqT9qohEREpkxKBiEjGKRGIiGScEoGISMYpEYiIZJwSgdQcM3s7Z8TJp5IaVTYa\nFXZUEu/dnWJiMrNtPb1eYP/5ZnZ5eZFJmvXd+y4iVfdm9DP9TDOzvjlj1IjERncEkgpmNszCvBPv\nitbvNrO/i57fbGZNuWPAR9vXmdl10V1Fk5kdaWZLzew5M7sw2ud4M3vczB6Kzn+LmeX9vzCzc6Ox\n5p8ys+9Hg4s1mNntZtZsYT6Jywocd3t0ziYzW21mp0fbG8zsm2a2LBpU7TM58fyXmS0BehxO3cwe\njAYqXNF1sEIz+3a0/VEzGx1tO8jMfh4d819mlopxcKQK3F2LlppagLeBp3KWj0XbTwZ+SxhK5Oc5\n+4+MHhuAXwGHRevrgL+Pnn8beAYYAowGXom2Hw+8BRwYHf8IcGbO8aOAGcDPgH7R9n8B/gY4Cngk\nJ47hBa7lduDnhC9d0wijTw4ELgCuivYZADQBU6N4tgNTu/m3WQeM6nLdgwjDL+wXrTvw19Hzq4Eb\no+ePAtOi50cDv4yezwcuT/pz15LcoqohqUUFq4bc/REzOwu4CTg856Wzo2/EfYFxwExCoQ8dY08t\nB/b1MN79VjPb2T4uDPC/7r4W3hl24jg6xo8HOJFQ6C8Lw+IwiDCI28+AA83se8BDwH92cz33eRg4\n7Y9mtpYwIuX7gcPMrH0MmmGERLEriuf5Hv+Fgs+a2Yej5xOj418jDNNwb7T934CfRCN7Hgv8OLoG\nCAlIRIlA0iOqspkB7ABGAC1mNhW4HPgzd99sZrcTvnG3ax/TpS3neft6+99/13FWuq4bcIe7X1kg\npsOBDwAXEmbU+tsCoRc6vwGXuPvSLuc7nnBH0KNov5OAY9x9h5n9is7X3fX9+hDGxc9824vkUxuB\npMllhNEcPw7cFg1dPJRQcL5hZmMJczr01uxoBNs+wMcIUwzmehQ408zGwDvzAU+Oeu/0cfcHgKuA\nI7s5/1lm1sfMDiJUQT0LLAX+ProGzGx6NPpssYYBm6MkcAjw5zmv9aFjtMuPA7/2ML7/89EdFRYc\njgi6I5DaNMjCLE/tfg7cBnwamO3uW83scUId+zwz+z3wB8Jsdb8p4f2WATcCBxOGDP5p7ovuvtLM\nriLMItcH2A1cBLxJSEjtX6jy7hgiLwD/S0haF7r7W2Z2KzAFeNJCXc0mejeF4c+BC81sFSGx/C7n\nte2E5HYVoQrrY9H2vwZujrb3I4yI+XQv3lPqlEYflUyLqlgud/fTYzr/7cD/c/f797avSFJUNSQi\nknG6IxARyTjdEYiIZJwSgYhIxikRiIhknBKBiEjGKRGIiGTc/weJmhL+XQWzQwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-JjG7v4EVKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}