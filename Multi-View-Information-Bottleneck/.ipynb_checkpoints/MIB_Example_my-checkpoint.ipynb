{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "\n",
    "# Flag to enable execution on GPU\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Loading the MNIST dataset\n",
    "train_set = MNIST('./data/MNIST', download=True, train=True, transform=ToTensor())\n",
    "test_set = MNIST('./data/MNIST', download=True, train=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomAffine, ToTensor\n",
    "from data_utils import PixelCorruption\n",
    "\n",
    "# Defining the augmentations\n",
    "t = Compose([\n",
    "    RandomAffine(degrees=15,\n",
    "                 translate=[0.1, 0.1],\n",
    "                 scale=[0.9, 1.1],\n",
    "                 shear=15), # Small affine transformations\n",
    "    ToTensor(),             # Conversion to torch tensor\n",
    "    PixelCorruption(0.8)    # PixelCorruption with keep probability 80%\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD1CAYAAABJE67gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADsVJREFUeJzt3XuQ1fV5x/HPwy5EEIKuBESLgMgCA15S1whqJWlCQimTDKWIxjQt2ukI3lJJJhGN0VQnt8ZWLnESW0WnBim2NoypF2pnVJRLvOFozILCStwuKFAQA0F299s/dukwfJ8DZ+Vw9jy779eMw+yH5/zOj/Hw4Tfn+z3nZyklAQDi6dHZJwAA+GgocAAIigIHgKAocAAIigIHgKAocAAIigJHpzKzW81sWwmOk8zsmhIcZ1j7saZ+hMc2tD/24P+2HO05AYVUd/YJAF3MzyUtOOjnDzvrRND1UeBAaTWllFZ39kmge+AtFFQ0MzvezBaaWb2Z7TGzTWa2yMw+7oz3MrO7zGyHme00swVm1uuQ451mZg+1z+wxsyfMbFSZ/jhASVHgqHR9JFVJuknSn0j6tqQ/lrTMmZ0r6Q8kXS7pdkl/I+mOA79pZjWSVkoaJekqSZdIOl7Sf5lZ70In0P4+fbHfOXGFmX1oZrvM7GEzG1rk44AO4y0UVLSU0nuSZh/42cyqJW2StNLMTkspbT5ofLekGSmlVkmPmdnHJN1kZt9LKe2Q9LdqK+xz2n+WmT0nqUHSFZIWFTiNVkktRZzuLyStlvSOpDGSviPpWTM7M6W0q9g/M1AsrsBR8czsL8zsZTP7QNJ+tV1FS1LtIaO/aC/vA/5dUm9J49p//pykFZLeN7Pq9n8Mdkt6UVJdoedPKX03pXTEi52U0vUppSUppWdTSj+T9AVJp0iadeQ/JdBxFDgqmplNk/SApFWSZkgaL2la+28fd8j4uwV+Htz+6wBJM9X2j8DB/31G0pCSnriklNJrkuol/WGpjw1IvIWCyjdD0pqU0pwDgZlNLDA7sMDPTe2/7pC0XNLfOY/dfTQneQR8ZzOOCQocla63pH2HZJcXmP2Smd140NsofyZpr6TX2n9+Sm0Ll6+nlPaW/EwPYWbj1LZg+tNj/VzonihwVIJeZvbnTv602t6zXmRmN0laI2mKpM8WOE4/ScvM7B5JYyXdImnhgQVLSXdK+oqk/zazBZIaJQ2SNFHSypTSEu+gZnaLpFsO9z64mf1p+7EflfQ/kkZLulnSZkmLCz0OOBoUOCpBP/nbAj+jtqvX0yVdr7b3vFdI+rLadnsc6sfts0vUtr7zT5LmHfjNlNI2Mxuvtq2F/yDpBLW9vbJS0quHOb8eatvKeDi/VdtbNv/Yftztkh6XNC+l9P4RHgt8JMYt1QAgJnahAEBQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQ1eV8skk9ZqRyPh+6nxWty6wznpfXdtf24G+fc/PLh1xYtnPwXttcgQNAUBQ4AARFgQNAUGV9DxwAJKl68MlFz9Z/fZibj5i7Osv2PenP9pmTL41subOXO/tS3dIs+8Ip/nvde6adnz/XI2vc2WOBK3AACIoCB4CgKHAACIoCB4CgKHAACIpdKABKxqrzSknNzVnW3LSl6GO+cekv3Xzq3HOz7IbhT7qzP1g4OcvWnPlzd3bT/r1ZNmxtb3e24VPl23Hi4QocAIKiwAEgKAocAIKiwAEgKBYxgS6sasxIN08f65llra/82p3dtOTsLBt+2Tp3dufyYVn27Nn5R9NL4ZF31mbZBT/8mjs7aP7zWTbhymvc2X6N+aJrr8d/1cGzKw+uwAEgKAocAIKiwAEgKAocAIKiwAEgKHahAF3Yjx57wM1re/o3M/C9mCVTlX+MXZL6T3kzDxuLf6ZpG6Zm2eIRD7uzu1rz+0h7u00KOemfVxV/YhWKK3AACIoCB4CgKHAACIoCB4CgWMQEurAXfz/EzWt7bi36GFNPzRcsH23MFzYLmds0Psvq6/a7s2nCwCz7sua4s7bK/zh/d8IVOAAERYEDQFAUOAAERYEDQFAUOAAExS4UoIuoHnxylj042p+9Y+mULKt6ta87u65xQdHncPu2s7Lszc/1cSZ3FX1MdpsUxhU4AARFgQNAUBQ4AARFgQNAUCxiAl1Ec9OWLNsxa4I7O+Lq9VnWsm27Ozt23BVZ9vrF97qzy382McsG7iz+O7pZsOwYrsABICgKHACCosABICgKHACCosABICh2oVQwq/b/96Tm5izzPkZdSP3Xh7l5S5/WLKudvbbo46Ly1Nzn33m9pQPHGH5ZvjOk0F3pxz7/6yx7b1EHngwdwhU4AARFgQNAUBQ4AARFgQNAUCxilkjVmJFZ1vLGBnd2y/UXZNne8b/Lspr+eSZJ/ae8mWX/8cIvj3SK/8+7y7jk32l80ojp7myfOZZlW+7s5c6+VLc0y7a1+H+285fNzbIzbljtzqLzfOL5E9z8vqFPZdnEmVdnWb+l/D8tBa7AASAoChwAgqLAASAoChwAgqLAASAodqGUyI8eeyDLanv6uzKkfLeHpyO7RTqi0OP3p/wD1jcMf9Kd/cHCyVn2icn17uymtz/Isu9vneTOnvJscnNUlu2zB7v55uV7s+xbt+d/N268ZJr7+PRy/ywbcof/dQDVw4dmWfPGBne2q+IKHACCosABICgKHACCosABICgWMUvkxd8PybLanluLfry3YFlosdGbHfVCT3d24wcDsmzxiIfd2V2t+QLiLQv+yp0dND+/0/j2K/07oF+VrzVJyhe7JKmP1rg5Kkvrujfc/NLbvpFlD37n77PslfH5wqYkTZ2ev7Y3fc9/XY28p+lwp9gtcAUOAEFR4AAQFAUOAEFR4AAQFAUOAEFZSuX76PKkHjO67Oekd8zKV8rfn+zftKDq1b5Ztm7Ogiwr9FH68ev2Z9nqs/1dKJ404Ww3b7guz7w7kleyFa3L8jtNlEFXfm0frXThOVn28e+/484uOf2JLCv09+DNf/lklp07bLM7u+ui7Yc7xRC81zZX4AAQFAUOAEFR4AAQFAUOAEGxiFkiVQNOyrKWbf7CyaYl+SKit1j47tX53eslaeCi/GPsaMMiZgxVgwa6ecvWd7NseeOv3NkezvXnnvShOzv12q9lWZ9HYn1tA4uYANCFUOAAEBQFDgBBUeAAEBTfB14ihRYsPfvfz2927H3396y3T3Af/96i4s8LqETeYmUhXzz1PDfvyM29oy1YFosrcAAIigIHgKAocAAIigIHgKAocAAIil0onWDMN9dn2awzP5tl9w19yn38xJlXZ1m/pauP/sSAY6D1ovz7wN+acZw7O+6chizbN3FL0c+1YEf+HeFdGVfgABAUBQ4AQVHgABAUBQ4AQbGI2Qladu7Ksu2zx2TZ5uV73cd/6/YHsuzGS6a5s+nl/lk25I5V/omV8bvhEZvVjXPz9dflXxMx8qsvZVn90uI/Bq9GPy50s+PuhCtwAAiKAgeAoChwAAiKAgeAoChwAAiKXSgVonXdG1l26W3fcGdr7s13kbzSmO9MkSSNz6Oxx1/jjo68pynLmjc2+MdFt1Fox4nH23Hi3Xih0A4Sb3be1jp39n//Mn9xn3h/gR1WXRRX4AAQFAUOAEFR4AAQFAUOAEGxiFnBvMVKSUoX5t+vfNnGAe7sktOfyLLXv7rQnR095K+zbNRt/r/xLRs2ujliqB4+NMvemnWKO3vrzIeybHrfbUU/l7dgOeqFnu7sBd/OF9hrFq91Z09s7V4Llh6uwAEgKAocAIKiwAEgKAocAIKiwAEgKHahBGTPvZJlu//Inz3vumuzbM0373Jnz/jKy1nWd+VJ7uyuiw5zgugU1cNOy7Lmhs3u7ORH89fQo2PfdmenX1n8jpMLbnZ2kSjfLVJft999vDeLwrgCB4CgKHAACIoCB4CgKHAACIpFzC5u0Pzns+yL889zZ73vYt6TPnRnL9GEozsxFKV68MlZtuPe493Z2cOfzrLL+m0t+rmuKnD3d881jaxiVwKuwAEgKAocAIKiwAEgKAocAIKiwAEgKHahBNR6UX5Dh7dmHOfOjjunIcv2TdxS9HMt2PHJomdRHs+c9a/H5LhbW/a6+cXL52bZ6Jt/487W7OSj8OXEFTgABEWBA0BQFDgABEWBA0BQLGKWiHeX7+ZN/vcrW924LFt/Xa8su+fC+93H/3BEntUvzT8GX8i+d/zvYp62YVqW7f90U9HHxUf3weOnu/m8M/6z6GMs2pm/MO56+vPurLVYlo28do07O1J53lL0WeFY4gocAIKiwAEgKAocAIKiwAEgKAocAIJiF0oH/W76+W4+87uPZ9mif5vizt4686Esm963+Dt/X+zceKGQeVvrsuzpu8a7syfez8egy2H93Z/KstrJa93Z+RrtZMWrlX9cdA1cgQNAUBQ4AARFgQNAUBQ4AATV7RYxj/4u38UvIF515cLiT6wD5jbli5CrfpIvVkpSzeJ8EevEVhYrO1PtbBYWURpcgQNAUBQ4AARFgQNAUBQ4AARFgQNAUF12F0qhL8jvO3ljlj1zVvE7S0ph6qnnZtmwtb2z7KW787vPS9KAh1/Lsprd7CwBuhuuwAEgKAocAIKiwAEgKAocAILqEouY3vcrrz/zbnd2qvIFxEKO9i7fz3zpx+7shoX5d4pXff43WVaz01+YbHVTAN0NV+AAEBQFDgBBUeAAEBQFDgBBUeAAEJSllMr2ZJN6zCjfk1WAqkED3bxl67tlPpPuY0Xrsnw7UBl0t9c2ys97bXMFDgBBUeAAEBQFDgBBUeAAEFRZFzEBAKXDFTgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABEWBA0BQFDgABPV/pxz2DVfPeoQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_utils import AugmentedDataset\n",
    "\n",
    "# Creating the multi-view dataset\n",
    "mv_train_set = AugmentedDataset(MNIST('./data/MNIST', train=True), t)\n",
    "\n",
    "# Visualization of one example\n",
    "f, ax = plt.subplots(1,2)\n",
    "\n",
    "idx = 0\n",
    "v_1, v_2, y = mv_train_set[idx]\n",
    "f.suptitle('Label: %d'%y,size=15)\n",
    "ax[0].imshow(v_1[0].data.numpy())\n",
    "ax[1].imshow(v_2[0].data.numpy())\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Initialization of the data loader\n",
    "train_loader = DataLoader(mv_train_set, batch_size=batch_size, shuffle=True, num_workers=8 if cuda else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Independent\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "# Encoder architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        # Vanilla MLP\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, z_dim*2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),-1) # Flatten the input\n",
    "        params = self.net(x)\n",
    "        \n",
    "        mu, sigma = params[:,:self.z_dim], params[:,self.z_dim:]\n",
    "        sigma = softplus(sigma) + 1e-7  # Make sigma always positive\n",
    "        \n",
    "        return Independent(Normal(loc=mu, scale=sigma), 1) # Return a factorized Normal distribution\n",
    "\n",
    "# Auxiliary network for mutual information estimation\n",
    "class MIEstimator(nn.Module):\n",
    "    def __init__(self, size1, size2):\n",
    "        super(MIEstimator, self).__init__()\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "        # Vanilla MLP\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(size1 + size2, 1024),\n",
    "            #nn.Linear(2 * np.max([size1, size2]), 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1),\n",
    "        )\n",
    "    \n",
    "    # Gradient for JSD mutual information estimation and EB-based estimation\n",
    "    def forward(self, x1, x2):\n",
    "        #Shape cast\n",
    "        #x2 = nn.ReLU(True)(nn.Linear(x2.size(-1), x1.size(-1))(x2))\n",
    "        \n",
    "        pos = self.net(torch.cat([x1, x2], 1)) #Positive Samples\n",
    "        neg = self.net(torch.cat([torch.roll(x1, 1, 0), x2], 1))\n",
    "        return -softplus(-pos).mean() - softplus(neg).mean(), pos.mean() - neg.exp().mean() + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the representation\n",
    "z_dim = 64\n",
    "x_dim = 28*28\n",
    "y_dim = 10\n",
    "\n",
    "# Intialization of the encoder(s)\n",
    "encoder_v_1 = Encoder(z_dim)\n",
    "encoder_v_2 = encoder_v_1 # Full parameter sharing for the two encoders\n",
    "\n",
    "# Initialization of the mutual information estimation network\n",
    "mi_estimator_X = MIEstimator(x_dim, z_dim) \n",
    "mi_estimator_Y = MIEstimator(z_dim, y_dim)\n",
    "\n",
    "# Moving the models to the GPU\n",
    "if cuda:\n",
    "    encoder_v_1 = encoder_v_1.cuda()\n",
    "    encoder_v_2 = encoder_v_2.cuda()\n",
    "    mi_estimator_X = mi_estimator_X.cuda()\n",
    "    mi_estimator_Y = mi_estimator_Y.cuda()\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Defining the optimizer\n",
    "opt = Adam([\n",
    "    {'params': encoder_v_1.parameters(), 'lr':1e-4},\n",
    "#     {'params': encoder_v_2.parameters(), 'lr':1e-4}, # There is only one encoder in this example\n",
    "    {'params': mi_estimator_X.parameters(), 'lr':1e-4},\n",
    "    {'params': mi_estimator_Y.parameters(), 'lr':1e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate, split\n",
    "\n",
    "\n",
    "# Select a subset (100 samples per label ~0.17 %) of labeled train points\n",
    "train_subset = split(train_set, 10, 'Balanced') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_train_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-91b0f3b0b055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_test_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# testing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_train_samples' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_samples(n_samples):\n",
    "    x_data = np.zeros((n_samples, 10)) # inputs\n",
    "    x_int = np.zeros(n_samples) # integers representing the inputs\n",
    "    y_data = np.zeros((n_samples, 2)) # outputs\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        random_int = randint(0, 1023)\n",
    "        x_data[i,:] = [int(b) for b in list(\"{0:b}\".format(random_int).zfill(10))]\n",
    "        x_int[i] = random_int\n",
    "        y_data[i,0] = groups[random_int % 16]\n",
    "        y_data[i,1] = 1 - y_data[i,0]\n",
    "        \n",
    "    return x_data, y_data, x_int\n",
    "\n",
    "\n",
    "x_train, y_train, x_train_int = generate_samples(n_train_samples) # training dataset\n",
    "x_test, y_test, _ = generate_samples(n_test_samples) # testing dataset\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(torch.Tensor(x_train).to(device), torch.Tensor(y_train).to(device)) \n",
    "test_set = torch.utils.data.TensorDataset(torch.Tensor(x_test).to(device), torch.Tensor(y_test).to(device))\n",
    "\n",
    "train_subset = split(train_set, 10, 'Balanced') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_utils import ExponentialScheduler\n",
    "\n",
    "# Defining the schedule for the update of the hyper-parameter over time\n",
    "beta_scheduler = ExponentialScheduler(start_value=1e-3, end_value=1, n_iterations=100000, start_iteration=50000)\n",
    "\n",
    "# Visualization of the scheduler behavior\n",
    "iterations = np.arange(250000)\n",
    "plt.plot(iterations, [beta_scheduler(iteration) for iteration in iterations])\n",
    "plt.title('$\\\\beta$ Schedule', size=20)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration', size=15)\n",
    "plt.ylabel('$\\\\beta$', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Training parameters\n",
    "# Increase number of epochs and delay the beta increment for better performances\n",
    "epochs = 300\n",
    "plot_every = 1\n",
    "mi_over_time = {'X':[], 'Y':[]}\n",
    "#skl_over_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426b3b09058d4b9ba108d1be0e3fc7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0017, grad_fn=<MeanBackward0>) tensor(-0.0022, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0040, grad_fn=<MeanBackward0>) tensor(-0.0011, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ntokoven/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/ntokoven/miniconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-0.0040, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-0.0106, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-0.0169, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0031, grad_fn=<MeanBackward0>) tensor(-0.0241, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0025, grad_fn=<MeanBackward0>) tensor(-0.0380, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, grad_fn=<MeanBackward0>) tensor(-0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, grad_fn=<MeanBackward0>) tensor(-0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, grad_fn=<MeanBackward0>) tensor(-0.1229, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-0.1577, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0036, grad_fn=<MeanBackward0>) tensor(-0.1902, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-0.2523, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0012, grad_fn=<MeanBackward0>) tensor(-0.3027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>) tensor(-0.3894, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0026, grad_fn=<MeanBackward0>) tensor(-0.4631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0027, grad_fn=<MeanBackward0>) tensor(-0.5766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0002, grad_fn=<MeanBackward0>) tensor(-0.6941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, grad_fn=<MeanBackward0>) tensor(-0.8162, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>) tensor(-0.9404, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, grad_fn=<MeanBackward0>) tensor(-1.0356, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0008, grad_fn=<MeanBackward0>) tensor(-1.2077, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>) tensor(-1.3469, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-1.4680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, grad_fn=<MeanBackward0>) tensor(-1.6558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, grad_fn=<MeanBackward0>) tensor(-1.8246, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0011, grad_fn=<MeanBackward0>) tensor(-2.0488, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0021, grad_fn=<MeanBackward0>) tensor(-2.1864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, grad_fn=<MeanBackward0>) tensor(-2.4700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0009, grad_fn=<MeanBackward0>) tensor(-2.5895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, grad_fn=<MeanBackward0>) tensor(-2.9069, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-3.0353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0015, grad_fn=<MeanBackward0>) tensor(-3.3396, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, grad_fn=<MeanBackward0>) tensor(-3.6576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0013, grad_fn=<MeanBackward0>) tensor(-3.9063, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0028, grad_fn=<MeanBackward0>) tensor(-4.2319, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0029, grad_fn=<MeanBackward0>) tensor(-4.4339, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0022, grad_fn=<MeanBackward0>) tensor(-4.7798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0036, grad_fn=<MeanBackward0>) tensor(-4.9994, grad_fn=<MeanBackward0>)\n",
      "tensor(8.4162e-05, grad_fn=<MeanBackward0>) tensor(-5.3853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, grad_fn=<MeanBackward0>) tensor(-5.6690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0032, grad_fn=<MeanBackward0>) tensor(-6.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0003, grad_fn=<MeanBackward0>) tensor(-6.4592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0039, grad_fn=<MeanBackward0>) tensor(-6.7461, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-7.2061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0060, grad_fn=<MeanBackward0>) tensor(-7.5605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0004, grad_fn=<MeanBackward0>) tensor(-8.1939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, grad_fn=<MeanBackward0>) tensor(-8.4257, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, grad_fn=<MeanBackward0>) tensor(-9.1738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0038, grad_fn=<MeanBackward0>) tensor(-9.6593, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-10.6388, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0084, grad_fn=<MeanBackward0>) tensor(-10.4118, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, grad_fn=<MeanBackward0>) tensor(-11.7846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0017, grad_fn=<MeanBackward0>) tensor(-12.2023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0044, grad_fn=<MeanBackward0>) tensor(-13.2306, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, grad_fn=<MeanBackward0>) tensor(-15.0702, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, grad_fn=<MeanBackward0>) tensor(-15.3338, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0049, grad_fn=<MeanBackward0>) tensor(-16.8625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, grad_fn=<MeanBackward0>) tensor(-18.8028, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0110, grad_fn=<MeanBackward0>) tensor(-21.2542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0076, grad_fn=<MeanBackward0>) tensor(-22.9188, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0045, grad_fn=<MeanBackward0>) tensor(-25.7854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0090, grad_fn=<MeanBackward0>) tensor(-28.7485, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0113, grad_fn=<MeanBackward0>) tensor(-33.3681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0110, grad_fn=<MeanBackward0>) tensor(-36.5651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0087, grad_fn=<MeanBackward0>) tensor(-43.9915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0074, grad_fn=<MeanBackward0>) tensor(-49.8564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0128, grad_fn=<MeanBackward0>) tensor(-59.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>) tensor(-68.3365, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0006, grad_fn=<MeanBackward0>) tensor(-77.4355, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0078, grad_fn=<MeanBackward0>) tensor(-86.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, grad_fn=<MeanBackward0>) tensor(-109.3165, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, grad_fn=<MeanBackward0>) tensor(-125.0053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0068, grad_fn=<MeanBackward0>) tensor(-146.1461, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0053, grad_fn=<MeanBackward0>) tensor(-185.1503, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0024, grad_fn=<MeanBackward0>) tensor(-217.4713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, grad_fn=<MeanBackward0>) tensor(-251.6205, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, grad_fn=<MeanBackward0>) tensor(-271.9440, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0035, grad_fn=<MeanBackward0>) tensor(-340.3436, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0031, grad_fn=<MeanBackward0>) tensor(-407.6797, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0139, grad_fn=<MeanBackward0>) tensor(-453.1328, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-548.2761, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0275, grad_fn=<MeanBackward0>) tensor(-656.8145, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0502, grad_fn=<MeanBackward0>) tensor(-752.6428, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0071, grad_fn=<MeanBackward0>) tensor(-902.7328, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0295, grad_fn=<MeanBackward0>) tensor(-957.4758, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0175, grad_fn=<MeanBackward0>) tensor(-1034.8680, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0239, grad_fn=<MeanBackward0>) tensor(-1294.0420, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0432, grad_fn=<MeanBackward0>) tensor(-1430.8114, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0276, grad_fn=<MeanBackward0>) tensor(-1706.6150, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0170, grad_fn=<MeanBackward0>) tensor(-1943.5845, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0370, grad_fn=<MeanBackward0>) tensor(-2088.6716, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0584, grad_fn=<MeanBackward0>) tensor(-2416.7568, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0318, grad_fn=<MeanBackward0>) tensor(-2860.2646, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0622, grad_fn=<MeanBackward0>) tensor(-3057.2944, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0823, grad_fn=<MeanBackward0>) tensor(-3720.3872, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.4494, grad_fn=<MeanBackward0>) tensor(-4174.7407, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.2892, grad_fn=<MeanBackward0>) tensor(-4390.5981, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0773, grad_fn=<MeanBackward0>) tensor(-5021.8804, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.8848, grad_fn=<MeanBackward0>) tensor(-5735.2861, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.1942, grad_fn=<MeanBackward0>) tensor(-6253.9321, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.0943, grad_fn=<MeanBackward0>) tensor(-7110.1655, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2357, grad_fn=<MeanBackward0>) tensor(-8134.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.1702, grad_fn=<MeanBackward0>) tensor(-9272.2803, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.2570, grad_fn=<MeanBackward0>) tensor(-10406.8848, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.8269, grad_fn=<MeanBackward0>) tensor(-12465.4375, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.2592, grad_fn=<MeanBackward0>) tensor(-11683.9268, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.4901, grad_fn=<MeanBackward0>) tensor(-14453.2910, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.5134, grad_fn=<MeanBackward0>) tensor(-14935.5889, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.4486, grad_fn=<MeanBackward0>) tensor(-18208.2461, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.6443, grad_fn=<MeanBackward0>) tensor(-19034.2188, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.3873, grad_fn=<MeanBackward0>) tensor(-21739.7930, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.1085, grad_fn=<MeanBackward0>) tensor(-22525.6211, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.5397, grad_fn=<MeanBackward0>) tensor(-26890.9668, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.2890, grad_fn=<MeanBackward0>) tensor(-29933.6641, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.8906, grad_fn=<MeanBackward0>) tensor(-32561.5898, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.2164, grad_fn=<MeanBackward0>) tensor(-34420.2812, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.9672, grad_fn=<MeanBackward0>) tensor(-39871.7930, grad_fn=<MeanBackward0>)\n",
      "tensor(-4.2307, grad_fn=<MeanBackward0>) tensor(-40765.2461, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.8454, grad_fn=<MeanBackward0>) tensor(-44168.7969, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.2073, grad_fn=<MeanBackward0>) tensor(-51154.5195, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.5548, grad_fn=<MeanBackward0>) tensor(-60770.8164, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.1859, grad_fn=<MeanBackward0>) tensor(-58133.8945, grad_fn=<MeanBackward0>)\n",
      "tensor(-32.8881, grad_fn=<MeanBackward0>) tensor(-68260.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.8613, grad_fn=<MeanBackward0>) tensor(-71831.6016, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.0924, grad_fn=<MeanBackward0>) tensor(-76025.4062, grad_fn=<MeanBackward0>)\n",
      "tensor(-60.0505, grad_fn=<MeanBackward0>) tensor(-90788.6953, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.2043, grad_fn=<MeanBackward0>) tensor(-92047.6953, grad_fn=<MeanBackward0>)\n",
      "tensor(-0.9732, grad_fn=<MeanBackward0>) tensor(-98707., grad_fn=<MeanBackward0>)\n",
      "tensor(-85406.9688, grad_fn=<MeanBackward0>) tensor(-118032.8125, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.0766, grad_fn=<MeanBackward0>) tensor(-116640.9688, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.8892, grad_fn=<MeanBackward0>) tensor(-122887.6562, grad_fn=<MeanBackward0>)\n",
      "tensor(-295.5145, grad_fn=<MeanBackward0>) tensor(-125652.6719, grad_fn=<MeanBackward0>)\n",
      "tensor(-15.9761, grad_fn=<MeanBackward0>) tensor(-148747.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.7545, grad_fn=<MeanBackward0>) tensor(-158394.8281, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.8855, grad_fn=<MeanBackward0>) tensor(-170859.6250, grad_fn=<MeanBackward0>)\n",
      "tensor(-2662.3743, grad_fn=<MeanBackward0>) tensor(-188212.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-19.8668, grad_fn=<MeanBackward0>) tensor(-216704.0469, grad_fn=<MeanBackward0>)\n",
      "tensor(-5.8782, grad_fn=<MeanBackward0>) tensor(-214605.7188, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.5280, grad_fn=<MeanBackward0>) tensor(-244670.7344, grad_fn=<MeanBackward0>)\n",
      "tensor(-625991.9375, grad_fn=<MeanBackward0>) tensor(-253735.3750, grad_fn=<MeanBackward0>)\n",
      "tensor(-1422778.6250, grad_fn=<MeanBackward0>) tensor(-277308.9375, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.9493, grad_fn=<MeanBackward0>) tensor(-302210.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.8810, grad_fn=<MeanBackward0>) tensor(-314575.3125, grad_fn=<MeanBackward0>)\n",
      "tensor(-12.5774, grad_fn=<MeanBackward0>) tensor(-329006.5312, grad_fn=<MeanBackward0>)\n",
      "tensor(-3894.0205, grad_fn=<MeanBackward0>) tensor(-344183.4688, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.8440, grad_fn=<MeanBackward0>) tensor(-387217.9062, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.4848, grad_fn=<MeanBackward0>) tensor(-400303.6250, grad_fn=<MeanBackward0>)\n",
      "tensor(-16121.2998, grad_fn=<MeanBackward0>) tensor(-462130.1875, grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8726, grad_fn=<MeanBackward0>) tensor(-470296.7812, grad_fn=<MeanBackward0>)\n",
      "tensor(-12.4118, grad_fn=<MeanBackward0>) tensor(-471301.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(-5.4398, grad_fn=<MeanBackward0>) tensor(-526468.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(-25.4553, grad_fn=<MeanBackward0>) tensor(-608135.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.5199, grad_fn=<MeanBackward0>) tensor(-566425.8125, grad_fn=<MeanBackward0>)\n",
      "tensor(-13013.3076, grad_fn=<MeanBackward0>) tensor(-613401., grad_fn=<MeanBackward0>)\n",
      "tensor(-2.8623, grad_fn=<MeanBackward0>) tensor(-677859., grad_fn=<MeanBackward0>)\n",
      "tensor(-137.3873, grad_fn=<MeanBackward0>) tensor(-682320.1875, grad_fn=<MeanBackward0>)\n",
      "tensor(-2.2020, grad_fn=<MeanBackward0>) tensor(-827471.8125, grad_fn=<MeanBackward0>)\n",
      "tensor(-75581.6328, grad_fn=<MeanBackward0>) tensor(-820764.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.0113, grad_fn=<MeanBackward0>) tensor(-896175., grad_fn=<MeanBackward0>)\n",
      "tensor(-2852.7087, grad_fn=<MeanBackward0>) tensor(-951939.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.0821, grad_fn=<MeanBackward0>) tensor(-954034.3750, grad_fn=<MeanBackward0>)\n",
      "tensor(-121304.4531, grad_fn=<MeanBackward0>) tensor(-1145232., grad_fn=<MeanBackward0>)\n",
      "tensor(-2.8650, grad_fn=<MeanBackward0>) tensor(-1141003.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(-78.2596, grad_fn=<MeanBackward0>) tensor(-1164190.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-53.5753, grad_fn=<MeanBackward0>) tensor(-1354036.6250, grad_fn=<MeanBackward0>)\n",
      "tensor(-5.1629, grad_fn=<MeanBackward0>) tensor(-1233173.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-62721.7422, grad_fn=<MeanBackward0>) tensor(-1343613.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1351, grad_fn=<MeanBackward0>) tensor(-1466815.3750, grad_fn=<MeanBackward0>)\n",
      "tensor(-419.5479, grad_fn=<MeanBackward0>) tensor(-1653128.3750, grad_fn=<MeanBackward0>)\n",
      "tensor(-349.9238, grad_fn=<MeanBackward0>) tensor(-1640548.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-11.9557, grad_fn=<MeanBackward0>) tensor(-1733097., grad_fn=<MeanBackward0>)\n",
      "tensor(-4.6085, grad_fn=<MeanBackward0>) tensor(-1909505.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-6.0375e+18, grad_fn=<MeanBackward0>) tensor(-2215352.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.1326e+21, grad_fn=<MeanBackward0>) tensor(-2139294.7500, grad_fn=<MeanBackward0>)\n",
      "tensor(-10.2505, grad_fn=<MeanBackward0>) tensor(-2188356., grad_fn=<MeanBackward0>)\n",
      "tensor(-18.5802, grad_fn=<MeanBackward0>) tensor(-2106167.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-17.7087, grad_fn=<MeanBackward0>) tensor(-2439145.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-283491.3438, grad_fn=<MeanBackward0>) tensor(-2557508.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-35650324., grad_fn=<MeanBackward0>) tensor(-2805375.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-13.6232, grad_fn=<MeanBackward0>) tensor(-2811293.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-11.3042, grad_fn=<MeanBackward0>) tensor(-2947442.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.0999e+12, grad_fn=<MeanBackward0>) tensor(-3289879.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-3.2167e+09, grad_fn=<MeanBackward0>) tensor(-3407848., grad_fn=<MeanBackward0>)\n",
      "tensor(-11.8799, grad_fn=<MeanBackward0>) tensor(-3302501., grad_fn=<MeanBackward0>)\n",
      "tensor(-13.2234, grad_fn=<MeanBackward0>) tensor(-3795539.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-2689200., grad_fn=<MeanBackward0>) tensor(-3712989.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-4.5352e+08, grad_fn=<MeanBackward0>) tensor(-4186837.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(-13.7314, grad_fn=<MeanBackward0>) tensor(-4011708., grad_fn=<MeanBackward0>)\n",
      "tensor(-13.6862, grad_fn=<MeanBackward0>) tensor(-4599361., grad_fn=<MeanBackward0>)\n",
      "tensor(-56078644., grad_fn=<MeanBackward0>) tensor(-4593114.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-5.9661e+09, grad_fn=<MeanBackward0>) tensor(-5013552., grad_fn=<MeanBackward0>)\n",
      "tensor(-12.0614, grad_fn=<MeanBackward0>) tensor(-5331616.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-8.1569, grad_fn=<MeanBackward0>) tensor(-5278320.5000, grad_fn=<MeanBackward0>)\n",
      "tensor(-1.5385e+15, grad_fn=<MeanBackward0>) tensor(-5573758., grad_fn=<MeanBackward0>)\n",
      "tensor(-1.9017e+12, grad_fn=<MeanBackward0>) tensor(-6065923., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.9031, grad_fn=<MeanBackward0>) tensor(-6381959., grad_fn=<MeanBackward0>)\n",
      "tensor(-10.5596, grad_fn=<MeanBackward0>) tensor(-6307521., grad_fn=<MeanBackward0>)\n",
      "tensor(-3.2917e+11, grad_fn=<MeanBackward0>) tensor(-6662979., grad_fn=<MeanBackward0>)\n",
      "tensor(-3.5041e+24, grad_fn=<MeanBackward0>) tensor(-7563051., grad_fn=<MeanBackward0>)\n",
      "tensor(-14.3782, grad_fn=<MeanBackward0>) tensor(-6531825., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-65cb8816718c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Accuracy: %f'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-65cb8816718c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_estimator_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e+3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Plot the loss components every 5 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def onehot(y):\n",
    "    y_onehot = torch.FloatTensor(y.shape[0], 10).to('cuda' if cuda else 'cpu')\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y.view(y.shape[0], 1), 1)\n",
    "    return y_onehot\n",
    "\n",
    "def train():\n",
    "    iterations = 0\n",
    "    # Training loop (approx 1h on GPU)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for v_1, v_2, label in train_loader:\n",
    "            if cuda:\n",
    "                v_1 = v_1.cuda()\n",
    "                #v_2 = v_2.cuda()\n",
    "\n",
    "            # Encode a batch of data\n",
    "            p_z_1_given_v_1 = encoder_v_1(v_1)\n",
    "            #p_z_2_given_v_2 = encoder_v_2(v_2)\n",
    "\n",
    "            # Sample from the posteriors with reparametrization\n",
    "            z_1 = p_z_1_given_v_1.rsample()\n",
    "            #z_2 = p_z_2_given_v_2.rsample()\n",
    "\n",
    "            # Mutual information estimation\n",
    "            mi_gradient_X, mi_estimation_X = mi_estimator_X(torch.flatten(v_1, start_dim=1), z_1)\n",
    "            mi_gradient_X = mi_gradient_X.mean()\n",
    "            mi_estimation_X = mi_estimation_X.mean()\n",
    "\n",
    "            mi_gradient_Y, mi_estimation_Y = mi_estimator_Y(z_1, onehot(label))\n",
    "            mi_gradient_Y = mi_gradient_Y.mean()\n",
    "            mi_estimation_Y = mi_estimation_Y.mean()\n",
    "\n",
    "            print(mi_estimation_X, mi_estimation_Y)\n",
    "\n",
    "            '''\n",
    "            # Symmetrized Kullback-Leibler divergence\n",
    "            kl_1_2 = p_z_1_given_v_1.log_prob(z_1) - p_z_2_given_v_2.log_prob(z_1)\n",
    "            kl_2_1 = p_z_2_given_v_2.log_prob(z_2) - p_z_1_given_v_1.log_prob(z_2)\n",
    "            skl = (kl_1_2 + kl_2_1).mean()/ 2.\n",
    "            '''\n",
    "\n",
    "            # Update the value of beta according to the policy\n",
    "            beta = beta_scheduler(iterations)\n",
    "            iterations +=1\n",
    "\n",
    "            # Computing the loss function\n",
    "            loss = - mi_gradient_X + beta * mi_gradient_Y#- mi_gradient + beta * skl\n",
    "\n",
    "            # Logging\n",
    "            mi_over_time['X'].append(mi_estimation_X.item())\n",
    "            mi_over_time['Y'].append(mi_estimation_Y.item())\n",
    "            #skl_over_time.append(skl.item())\n",
    "\n",
    "            # Backward pass and update\n",
    "            opt.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm(mi_estimator_Y.parameters(), max_norm=1e+3)\n",
    "            torch.nn.utils.clip_grad_norm(mi_estimator_X.parameters(), max_norm=1e+3)\n",
    "            loss.backward()\n",
    "            opt.step()   \n",
    "\n",
    "        # Plot the loss components every 5 epochs\n",
    "        if epoch % plot_every == 0:\n",
    "            f, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "            ax[0].set_title('$I(z_1;z_2)$')\n",
    "            ax[1].set_title('$D_{SKL}(p(z_1|v_1)||p(z_2|v_2))$')\n",
    "            ax[1].set_yscale('log')\n",
    "            ax[0].plot(mi_over_time, '.', alpha=0.1)\n",
    "            ax[1].plot(skl_over_time, '.r', alpha=0.1)\n",
    "            ax[0].set_ylim(0,8)\n",
    "            ax[1].set_ylim(1e-3)\n",
    "\n",
    "            f.suptitle('Epoch: %d'%epoch, fontsize=15)\n",
    "            plt.show()\n",
    "\n",
    "            # Compute train and test_accuracy of a logistic regression\n",
    "            train_accuracy, test_accuracy = evaluate(encoder=encoder_v_1, train_on=train_subset, test_on=test_set, cuda=cuda)\n",
    "            print('Train Accuracy: %f'% train_accuracy)\n",
    "            print('Test Accuracy: %f'% test_accuracy)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.000000\n",
      "Test Accuracy: 0.364350\n"
     ]
    }
   ],
   "source": [
    "# Testing with 1 sample per label (~0.0017 % of labeled train points)\n",
    "train_smallest_subset = split(train_set, 10, 'Balanced') \n",
    "\n",
    "train_accuracy, test_accuracy = evaluate(encoder=encoder_v_1, train_on=train_smallest_subset, test_on=test_set, cuda=cuda)\n",
    "print('Train Accuracy: %f'% train_accuracy)\n",
    "print('Test Accuracy: %f'% test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
